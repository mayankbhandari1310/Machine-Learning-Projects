{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "face_recog_data = pd.read_csv('fer2013.csv')\n",
    "face_recog_data.head()\n",
    "\n",
    "train_data = face_recog_data[[\"emotion\", \"pixels\"]][face_recog_data[\"Usage\"] == \"Training\"]\n",
    "test_data = face_recog_data[[\"emotion\", \"pixels\"]][face_recog_data[\"Usage\"] == \"PublicTest\"]\n",
    "\n",
    "#loading training data\n",
    "training_target_values = np.array(train_data['emotion'])\n",
    "training_target_values = training_target_values.astype(np.float32)\n",
    "training_input_face_images = np.vstack((train_data['pixels'].apply(lambda pixel : np.fromstring(pixel, sep=' '))))\n",
    "training_input_face_images = np.reshape(training_input_face_images, (training_input_face_images.shape[0], 48, 48))\n",
    "training_input_face_images = training_input_face_images.astype(np.float32)\n",
    "training_input_face_images = training_input_face_images\n",
    "\n",
    "#loading test data\n",
    "testing_target_values = np.array(test_data['emotion'])\n",
    "testing_target_values = testing_target_values.astype(np.float32)\n",
    "testing_input_face_images = np.vstack((test_data['pixels'].apply(lambda pixel : np.fromstring(pixel, sep=' '))))\n",
    "testing_input_face_images = np.reshape(testing_input_face_images, (testing_input_face_images.shape[0], 48, 48))\n",
    "testing_input_face_images = testing_input_face_images.astype(np.float32)\n",
    "testing_input_face_images = testing_input_face_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28709, 48, 48) (28709,)\n",
      "(3589, 48, 48) (3589,)\n"
     ]
    }
   ],
   "source": [
    "print(training_input_face_images.shape, training_target_values.shape)\n",
    "print(testing_input_face_images.shape, testing_target_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_length = 28709\n",
    "#split = int(data_length * 0.8)\n",
    "\n",
    "\n",
    "\n",
    "X_train = training_input_face_images\n",
    "X_val = testing_input_face_images\n",
    "\n",
    "y_train = np_utils.to_categorical(training_target_values[:])\n",
    "y_val = np_utils.to_categorical(training_target_values)\n",
    "\n",
    "#print(X_train.shape, X_val.shape)\n",
    "#print(y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'[ 1.  0.  0.  0.  0.  0.  0.]')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE/CAYAAAAnhFRiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a245cf2eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[5, 5])\n",
    "plt.imshow(X_train[0], cmap='gray')\n",
    "plt.title(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3589, 7) (3589, 48, 48, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "#X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "X_test = testing_input_face_images.reshape(-1, 48,48, 1)\n",
    "y_test = testing_target_values\n",
    "y_test = np_utils.to_categorical(testing_target_values)\n",
    "print(y_test.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "#X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "#X_train = X_train/255.0\n",
    "#X_val = X_val/255.0\n",
    "#X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "num_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "#Deep model with less number of layers, lesser max-pools and small dense layers\n",
    "#1st convolution layer\n",
    "model1.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model1.add(BatchNormalization())\n",
    "model1.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model1.add(Flatten())\n",
    " \n",
    "#fully connected layers\n",
    "model1.add(Dense(512, activation='relu'))\n",
    "model1.add(Dropout(0.75))\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "\n",
    "#1st convolution layer\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model2.add(Flatten())\n",
    " \n",
    "#fully connected layers\n",
    "model2.add(Dense(512, activation='relu'))\n",
    "model2.add(Dropout(0.6))\n",
    "model2.add(Dense(256, activation='relu'))\n",
    "model2.add(Dropout(0.6))\n",
    "model2.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "\n",
    "#1st convolution layer\n",
    "model3.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model3.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "#model3.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model3.add(Dropout(0.5))\n",
    "\n",
    "model3.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.4))\n",
    "\n",
    "model3.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.4))\n",
    "\n",
    "model3.add(Flatten())\n",
    " \n",
    "#fully connected layers\n",
    "model3.add(Dense(1024, activation='relu'))\n",
    "model3.add(Dropout(0.75))\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "\n",
    "#1st convolution layer\n",
    "model4.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model4.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "model4.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "#model4.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.6))\n",
    "\n",
    "model4.add(Conv2D(256, (5, 5), activation='relu'))\n",
    "#model4.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model4.add(BatchNormalization())\n",
    "model4.add(Dropout(0.6))\n",
    "\n",
    "\n",
    "model4.add(Flatten())\n",
    " \n",
    "#fully connected layers\n",
    "model4.add(Dense(512, activation='relu'))\n",
    "model4.add(Dropout(0.6))\n",
    "model4.add(Dense(128, activation='relu'))\n",
    "model4.add(Dropout(0.6))\n",
    "model4.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model5 = Sequential()\n",
    "\n",
    "\n",
    "#1st convolution layer\n",
    "model5.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model5.add(BatchNormalization())\n",
    "#model5.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model5.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model5.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model5.add(BatchNormalization())\n",
    "#model5.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model5.add(Dropout(0.5))\n",
    "\n",
    "model5.add(Conv2D(128, (5, 5), activation='relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "\n",
    "model5.add(Conv2D(256, (5, 5), activation='relu'))\n",
    "model5.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model5.add(BatchNormalization())\n",
    "model5.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model5.add(Flatten())\n",
    " \n",
    "#fully connected layers\n",
    "model5.add(Dense(512, activation='relu'))\n",
    "model5.add(Dropout(0.75))\n",
    "model5.add(Dense(128, activation='relu'))\n",
    "model5.add(Dropout(0.5))\n",
    "model5.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model6 = Sequential()\n",
    "\n",
    "\n",
    "#1st convolution layer\n",
    "model6.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model6.add(BatchNormalization())\n",
    "#model6.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model6.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model6.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "#model6.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model6.add(Dropout(0.75))\n",
    "\n",
    "model6.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "#model6.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model6.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model6.add(Dropout(0.75))\n",
    "\n",
    "model6.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model6.add(BatchNormalization())\n",
    "model6.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model6.add(Dropout(0.75))\n",
    "\n",
    "model6.add(Flatten())\n",
    " \n",
    "#fully connected layers\n",
    "model6.add(Dense(512, activation='relu'))\n",
    "model6.add(Dropout(0.5))\n",
    "model6.add(Dense(128, activation='relu'))\n",
    "model6.add(Dropout(0.5))\n",
    "model6.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_list = [model1, model2, model3, model4, model5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ix in range(len(model_list)):\n",
    "    model_list[ix].compile(loss=keras.losses.categorical_crossentropy, \n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_115 (Conv2D)          (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_115 (Bat (None, 46, 46, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 46, 46, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_116 (Conv2D)          (None, 44, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_116 (Bat (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_117 (Conv2D)          (None, 42, 42, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_117 (Bat (None, 42, 42, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_81 (Dropout)         (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_118 (Conv2D)          (None, 40, 40, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_118 (Bat (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_119 (Conv2D)          (None, 38, 38, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_119 (Bat (None, 19, 19, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_82 (Dropout)         (None, 19, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_120 (Conv2D)          (None, 17, 17, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_120 (Bat (None, 17, 17, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_121 (Bat (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_83 (Dropout)         (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dropout_84 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_85 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 1,877,831.0\n",
      "Trainable params: 1,876,999.0\n",
      "Non-trainable params: 832.0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model  2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_122 (Conv2D)          (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_122 (Bat (None, 46, 46, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_123 (Bat (None, 23, 23, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_86 (Dropout)         (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          (None, 21, 21, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_124 (Bat (None, 21, 21, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 21, 21, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          (None, 19, 19, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_125 (Bat (None, 19, 19, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 19, 19, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          (None, 17, 17, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_126 (Bat (None, 17, 17, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 6, 6, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_127 (Bat (None, 6, 6, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_90 (Dropout)         (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_128 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_91 (Dropout)         (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_129 (Bat (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_92 (Dropout)         (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_93 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_94 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 386,535.0\n",
      "Trainable params: 385,831.0\n",
      "Non-trainable params: 704.0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model  3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_129 (Conv2D)          (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_130 (Bat (None, 46, 46, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_130 (Conv2D)          (None, 44, 44, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_131 (Bat (None, 44, 44, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 22, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_95 (Dropout)         (None, 22, 22, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_131 (Conv2D)          (None, 20, 20, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_132 (Bat (None, 20, 20, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_132 (Conv2D)          (None, 18, 18, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_133 (Bat (None, 18, 18, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_96 (Dropout)         (None, 18, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_133 (Conv2D)          (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_134 (Bat (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_134 (Conv2D)          (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_135 (Bat (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_136 (Bat (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_97 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_135 (Conv2D)          (None, 5, 5, 512)         590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_137 (Bat (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 3, 3, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 1, 1, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_98 (Dropout)         (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_100 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_101 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 4,049,639.0\n",
      "Trainable params: 4,045,415.0\n",
      "Non-trainable params: 4,224.0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model  4\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_137 (Conv2D)          (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_140 (Bat (None, 46, 46, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_102 (Dropout)        (None, 23, 23, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_138 (Conv2D)          (None, 21, 21, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_141 (Bat (None, 21, 21, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_103 (Dropout)        (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_139 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_142 (Bat (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_140 (Conv2D)          (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_143 (Bat (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_106 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_107 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 3,078,279.0\n",
      "Trainable params: 3,077,319.0\n",
      "Non-trainable params: 960.0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model  5\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_141 (Conv2D)          (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_144 (Bat (None, 46, 46, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_108 (Dropout)        (None, 46, 46, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_142 (Conv2D)          (None, 44, 44, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_145 (Bat (None, 44, 44, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_109 (Dropout)        (None, 44, 44, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_143 (Conv2D)          (None, 40, 40, 128)       204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_146 (Bat (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_110 (Dropout)        (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_144 (Conv2D)          (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_147 (Bat (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_111 (Dropout)        (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 512)               8389120   \n",
      "_________________________________________________________________\n",
      "dropout_112 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 9,500,807.0\n",
      "Trainable params: 9,499,847.0\n",
      "Non-trainable params: 960.0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model  6\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_145 (Conv2D)          (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 46, 46, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_146 (Conv2D)          (None, 44, 44, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 44, 44, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_147 (Conv2D)          (None, 42, 42, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_150 (Bat (None, 42, 42, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_148 (Conv2D)          (None, 40, 40, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_151 (Bat (None, 40, 40, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_114 (Dropout)        (None, 40, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_149 (Conv2D)          (None, 38, 38, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_152 (Bat (None, 38, 38, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_150 (Conv2D)          (None, 36, 36, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_153 (Bat (None, 36, 36, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_151 (Conv2D)          (None, 34, 34, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_154 (Bat (None, 34, 34, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_152 (Conv2D)          (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_155 (Bat (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_115 (Dropout)        (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_153 (Conv2D)          (None, 30, 30, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_156 (Bat (None, 30, 30, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_154 (Conv2D)          (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_157 (Bat (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_155 (Conv2D)          (None, 26, 26, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_158 (Bat (None, 26, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_156 (Conv2D)          (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_159 (Bat (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_116 (Dropout)        (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_157 (Conv2D)          (None, 10, 10, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_160 (Bat (None, 10, 10, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_158 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_161 (Bat (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_159 (Conv2D)          (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_162 (Bat (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_160 (Conv2D)          (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_163 (Bat (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_117 (Dropout)        (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_118 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 3,338,407.0\n",
      "Trainable params: 3,334,567.0\n",
      "Non-trainable params: 3,840.0\n",
      "_________________________________________________________________\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ix in range(len(model_list)):\n",
    "    print('Model ', ix+1)\n",
    "    model_list[ix].summary()\n",
    "    print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpointer = []\n",
    "train_model = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "face_model_path = 'face_model' + str(1) + '.h5'\n",
    "checkpointer.append(ModelCheckpoint(filepath=face_model_path, verbose=1, save_best_only=True))\n",
    "face_model_path = 'face_model' + str(2) + '.h5'\n",
    "checkpointer.append(ModelCheckpoint(filepath=face_model_path, verbose=1, save_best_only=True))\n",
    "face_model_path = 'face_model' + str(3) + '.h5'\n",
    "checkpointer.append(ModelCheckpoint(filepath=face_model_path, verbose=1, save_best_only=True))\n",
    "face_model_path = 'face_model' + str(4) + '.h5'\n",
    "checkpointer.append(ModelCheckpoint(filepath=face_model_path, verbose=1, save_best_only=True))\n",
    "face_model_path = 'face_model' + str(5) + '.h5'\n",
    "checkpointer.append(ModelCheckpoint(filepath=face_model_path, verbose=1, save_best_only=True))\n",
    "face_model_path = 'face_model' + str(6) + '.h5'\n",
    "checkpointer.append(ModelCheckpoint(filepath=face_model_path, verbose=1, save_best_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model no : 1\n",
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 2.4122 - acc: 0.2064Epoch 00000: val_loss improved from inf to 1.80333, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 88s - loss: 2.4116 - acc: 0.2065 - val_loss: 1.8033 - val_acc: 0.2494\n",
      "Epoch 2/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.8851 - acc: 0.2419Epoch 00001: val_loss improved from 1.80333 to 1.76609, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.8850 - acc: 0.2418 - val_loss: 1.7661 - val_acc: 0.2502\n",
      "Epoch 3/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7996 - acc: 0.2671Epoch 00002: val_loss improved from 1.76609 to 1.71233, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.7995 - acc: 0.2671 - val_loss: 1.7123 - val_acc: 0.2853\n",
      "Epoch 4/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7218 - acc: 0.3057Epoch 00003: val_loss improved from 1.71233 to 1.61820, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.7217 - acc: 0.3057 - val_loss: 1.6182 - val_acc: 0.3502\n",
      "Epoch 5/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6693 - acc: 0.3387Epoch 00004: val_loss improved from 1.61820 to 1.60832, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.6691 - acc: 0.3387 - val_loss: 1.6083 - val_acc: 0.3553\n",
      "Epoch 6/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6223 - acc: 0.3593Epoch 00005: val_loss improved from 1.60832 to 1.55606, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.6225 - acc: 0.3594 - val_loss: 1.5561 - val_acc: 0.3709\n",
      "Epoch 7/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5672 - acc: 0.3819Epoch 00006: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.5675 - acc: 0.3818 - val_loss: 1.5825 - val_acc: 0.3647\n",
      "Epoch 8/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5350 - acc: 0.3958Epoch 00007: val_loss improved from 1.55606 to 1.43685, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.5350 - acc: 0.3958 - val_loss: 1.4369 - val_acc: 0.4333\n",
      "Epoch 9/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5012 - acc: 0.4100Epoch 00008: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 1.5012 - acc: 0.4100 - val_loss: 1.4597 - val_acc: 0.4324\n",
      "Epoch 10/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4768 - acc: 0.4270Epoch 00009: val_loss improved from 1.43685 to 1.43182, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.4769 - acc: 0.4268 - val_loss: 1.4318 - val_acc: 0.4536\n",
      "Epoch 11/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4561 - acc: 0.4356Epoch 00010: val_loss improved from 1.43182 to 1.37878, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.4558 - acc: 0.4357 - val_loss: 1.3788 - val_acc: 0.4712\n",
      "Epoch 12/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4276 - acc: 0.4479Epoch 00011: val_loss improved from 1.37878 to 1.32848, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.4275 - acc: 0.4478 - val_loss: 1.3285 - val_acc: 0.4834\n",
      "Epoch 13/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4147 - acc: 0.4565Epoch 00012: val_loss improved from 1.32848 to 1.31406, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.4148 - acc: 0.4564 - val_loss: 1.3141 - val_acc: 0.4893\n",
      "Epoch 14/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3892 - acc: 0.4630Epoch 00013: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.3895 - acc: 0.4629 - val_loss: 1.3214 - val_acc: 0.4804\n",
      "Epoch 15/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3783 - acc: 0.4696Epoch 00014: val_loss improved from 1.31406 to 1.30452, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.3782 - acc: 0.4696 - val_loss: 1.3045 - val_acc: 0.5068\n",
      "Epoch 16/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3551 - acc: 0.4743Epoch 00015: val_loss improved from 1.30452 to 1.27598, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.3553 - acc: 0.4742 - val_loss: 1.2760 - val_acc: 0.5043\n",
      "Epoch 17/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3447 - acc: 0.4809Epoch 00016: val_loss improved from 1.27598 to 1.25984, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.3451 - acc: 0.4806 - val_loss: 1.2598 - val_acc: 0.5021\n",
      "Epoch 18/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3330 - acc: 0.4870Epoch 00017: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.3332 - acc: 0.4868 - val_loss: 1.2601 - val_acc: 0.5188\n",
      "Epoch 19/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3204 - acc: 0.4917Epoch 00018: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.3203 - acc: 0.4917 - val_loss: 1.2880 - val_acc: 0.5007\n",
      "Epoch 20/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2996 - acc: 0.4983Epoch 00019: val_loss improved from 1.25984 to 1.22277, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.2999 - acc: 0.4981 - val_loss: 1.2228 - val_acc: 0.5266\n",
      "Epoch 21/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2977 - acc: 0.4984Epoch 00020: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 1.2979 - acc: 0.4984 - val_loss: 1.2299 - val_acc: 0.5261\n",
      "Epoch 22/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2850 - acc: 0.5059Epoch 00021: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.2850 - acc: 0.5059 - val_loss: 1.2785 - val_acc: 0.5015\n",
      "Epoch 23/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2811 - acc: 0.5131Epoch 00022: val_loss improved from 1.22277 to 1.20156, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 68s - loss: 1.2809 - acc: 0.5133 - val_loss: 1.2016 - val_acc: 0.5400\n",
      "Epoch 24/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2599 - acc: 0.5206Epoch 00023: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 1.2600 - acc: 0.5206 - val_loss: 1.2139 - val_acc: 0.5344\n",
      "Epoch 25/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2567 - acc: 0.5226Epoch 00024: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 1.2567 - acc: 0.5228 - val_loss: 1.2372 - val_acc: 0.5272\n",
      "Epoch 26/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2497 - acc: 0.5245Epoch 00025: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 1.2496 - acc: 0.5245 - val_loss: 1.2301 - val_acc: 0.5274\n",
      "Epoch 27/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2316 - acc: 0.5307Epoch 00026: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.2315 - acc: 0.5306 - val_loss: 1.2042 - val_acc: 0.5389\n",
      "Epoch 28/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2222 - acc: 0.5321Epoch 00027: val_loss improved from 1.20156 to 1.16071, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.2224 - acc: 0.5322 - val_loss: 1.1607 - val_acc: 0.5578\n",
      "Epoch 29/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2145 - acc: 0.5373Epoch 00028: val_loss improved from 1.16071 to 1.14644, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.2147 - acc: 0.5373 - val_loss: 1.1464 - val_acc: 0.5620\n",
      "Epoch 30/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2073 - acc: 0.5435Epoch 00029: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.2070 - acc: 0.5437 - val_loss: 1.2307 - val_acc: 0.5375\n",
      "Epoch 31/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1932 - acc: 0.5480Epoch 00030: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 1.1934 - acc: 0.5480 - val_loss: 1.1486 - val_acc: 0.5623\n",
      "Epoch 32/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1898 - acc: 0.5498Epoch 00031: val_loss improved from 1.14644 to 1.13348, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.1897 - acc: 0.5499 - val_loss: 1.1335 - val_acc: 0.5731\n",
      "Epoch 33/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1693 - acc: 0.5591Epoch 00032: val_loss improved from 1.13348 to 1.12067, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.1694 - acc: 0.5590 - val_loss: 1.1207 - val_acc: 0.5704\n",
      "Epoch 34/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1661 - acc: 0.5602Epoch 00033: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.1664 - acc: 0.5600 - val_loss: 1.1494 - val_acc: 0.5561\n",
      "Epoch 35/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1662 - acc: 0.5597Epoch 00034: val_loss improved from 1.12067 to 1.10789, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.1661 - acc: 0.5599 - val_loss: 1.1079 - val_acc: 0.5784\n",
      "Epoch 36/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1509 - acc: 0.5626Epoch 00035: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.1511 - acc: 0.5626 - val_loss: 1.1410 - val_acc: 0.5653\n",
      "Epoch 37/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1409 - acc: 0.5686Epoch 00036: val_loss improved from 1.10789 to 1.09659, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.1410 - acc: 0.5685 - val_loss: 1.0966 - val_acc: 0.5812\n",
      "Epoch 38/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1322 - acc: 0.5731Epoch 00037: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.1322 - acc: 0.5733 - val_loss: 1.1283 - val_acc: 0.5754\n",
      "Epoch 39/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1223 - acc: 0.5752Epoch 00038: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 1.1227 - acc: 0.5748 - val_loss: 1.1205 - val_acc: 0.5595\n",
      "Epoch 40/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1201 - acc: 0.5783Epoch 00039: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.1201 - acc: 0.5782 - val_loss: 1.1013 - val_acc: 0.5804\n",
      "Epoch 41/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1167 - acc: 0.5799Epoch 00040: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 1.1169 - acc: 0.5798 - val_loss: 1.1075 - val_acc: 0.5793\n",
      "Epoch 42/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1078 - acc: 0.5831Epoch 00041: val_loss improved from 1.09659 to 1.09472, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.1074 - acc: 0.5832 - val_loss: 1.0947 - val_acc: 0.5795\n",
      "Epoch 43/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0936 - acc: 0.5857Epoch 00042: val_loss improved from 1.09472 to 1.07608, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.0934 - acc: 0.5859 - val_loss: 1.0761 - val_acc: 0.5899\n",
      "Epoch 44/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0801 - acc: 0.5931Epoch 00043: val_loss improved from 1.07608 to 1.06887, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.0803 - acc: 0.5931 - val_loss: 1.0689 - val_acc: 0.5954\n",
      "Epoch 45/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0749 - acc: 0.5941Epoch 00044: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 1.0747 - acc: 0.5940 - val_loss: 1.0860 - val_acc: 0.5890\n",
      "Epoch 46/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0664 - acc: 0.5970Epoch 00045: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.0660 - acc: 0.5972 - val_loss: 1.0927 - val_acc: 0.5887\n",
      "Epoch 47/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0623 - acc: 0.5986Epoch 00046: val_loss improved from 1.06887 to 1.05880, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.0621 - acc: 0.5987 - val_loss: 1.0588 - val_acc: 0.5960\n",
      "Epoch 48/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0540 - acc: 0.6061Epoch 00047: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.0539 - acc: 0.6061 - val_loss: 1.1158 - val_acc: 0.5765\n",
      "Epoch 49/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0471 - acc: 0.6071Epoch 00048: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.0472 - acc: 0.6071 - val_loss: 1.0796 - val_acc: 0.6007\n",
      "Epoch 50/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0348 - acc: 0.6085Epoch 00049: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.0350 - acc: 0.6085 - val_loss: 1.0754 - val_acc: 0.5979\n",
      "Epoch 51/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0344 - acc: 0.6084Epoch 00050: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.0339 - acc: 0.6086 - val_loss: 1.1071 - val_acc: 0.5823\n",
      "Epoch 52/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0299 - acc: 0.6121Epoch 00051: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.0301 - acc: 0.6121 - val_loss: 1.1296 - val_acc: 0.5854\n",
      "Epoch 53/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0270 - acc: 0.6158Epoch 00052: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.0271 - acc: 0.6156 - val_loss: 1.1703 - val_acc: 0.5578\n",
      "Epoch 54/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0140 - acc: 0.6185Epoch 00053: val_loss improved from 1.05880 to 1.04446, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.0142 - acc: 0.6186 - val_loss: 1.0445 - val_acc: 0.6119\n",
      "Epoch 55/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0137 - acc: 0.6212Epoch 00054: val_loss improved from 1.04446 to 1.03870, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 1.0135 - acc: 0.6211 - val_loss: 1.0387 - val_acc: 0.6091\n",
      "Epoch 56/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0013 - acc: 0.6244Epoch 00055: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 1.0012 - acc: 0.6245 - val_loss: 1.0617 - val_acc: 0.5977\n",
      "Epoch 57/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9980 - acc: 0.6241Epoch 00056: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9978 - acc: 0.6242 - val_loss: 1.0527 - val_acc: 0.6082\n",
      "Epoch 58/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9986 - acc: 0.6241Epoch 00057: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9986 - acc: 0.6242 - val_loss: 1.0838 - val_acc: 0.5846\n",
      "Epoch 59/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9924 - acc: 0.6278Epoch 00058: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9925 - acc: 0.6278 - val_loss: 1.0464 - val_acc: 0.6119\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9780 - acc: 0.6325Epoch 00059: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9784 - acc: 0.6323 - val_loss: 1.0886 - val_acc: 0.5893\n",
      "Epoch 61/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9718 - acc: 0.6333Epoch 00060: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 0.9716 - acc: 0.6335 - val_loss: 1.0399 - val_acc: 0.6177\n",
      "Epoch 62/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9814 - acc: 0.6342Epoch 00061: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9812 - acc: 0.6343 - val_loss: 1.0590 - val_acc: 0.5993\n",
      "Epoch 63/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9666 - acc: 0.6393Epoch 00062: val_loss improved from 1.03870 to 1.02457, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 0.9665 - acc: 0.6393 - val_loss: 1.0246 - val_acc: 0.6172\n",
      "Epoch 64/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9636 - acc: 0.6360Epoch 00063: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9640 - acc: 0.6360 - val_loss: 1.0496 - val_acc: 0.6110\n",
      "Epoch 65/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9603 - acc: 0.6430Epoch 00064: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9599 - acc: 0.6431 - val_loss: 1.0256 - val_acc: 0.6138\n",
      "Epoch 66/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9473 - acc: 0.6428Epoch 00065: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9476 - acc: 0.6426 - val_loss: 1.0374 - val_acc: 0.6186\n",
      "Epoch 67/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9513 - acc: 0.6404Epoch 00066: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 0.9514 - acc: 0.6404 - val_loss: 1.0250 - val_acc: 0.6155\n",
      "Epoch 68/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9406 - acc: 0.6420Epoch 00067: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9407 - acc: 0.6420 - val_loss: 1.0522 - val_acc: 0.6038\n",
      "Epoch 69/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9400 - acc: 0.6469Epoch 00068: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9402 - acc: 0.6468 - val_loss: 1.0278 - val_acc: 0.6213\n",
      "Epoch 70/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9350 - acc: 0.6469Epoch 00069: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9351 - acc: 0.6468 - val_loss: 1.0358 - val_acc: 0.6186\n",
      "Epoch 71/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9358 - acc: 0.6492Epoch 00070: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9357 - acc: 0.6493 - val_loss: 1.0296 - val_acc: 0.6252\n",
      "Epoch 72/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9232 - acc: 0.6543Epoch 00071: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9234 - acc: 0.6543 - val_loss: 1.0866 - val_acc: 0.6102\n",
      "Epoch 73/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9260 - acc: 0.6516Epoch 00072: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9259 - acc: 0.6517 - val_loss: 1.0333 - val_acc: 0.6127\n",
      "Epoch 74/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9100 - acc: 0.6600Epoch 00073: val_loss improved from 1.02457 to 1.02061, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 0.9099 - acc: 0.6600 - val_loss: 1.0206 - val_acc: 0.6236\n",
      "Epoch 75/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9181 - acc: 0.6531Epoch 00074: val_loss improved from 1.02061 to 1.01943, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 0.9181 - acc: 0.6532 - val_loss: 1.0194 - val_acc: 0.6239\n",
      "Epoch 76/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9112 - acc: 0.6576Epoch 00075: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9109 - acc: 0.6578 - val_loss: 1.0231 - val_acc: 0.6272\n",
      "Epoch 77/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9075 - acc: 0.6566Epoch 00076: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9072 - acc: 0.6567 - val_loss: 1.0567 - val_acc: 0.6158\n",
      "Epoch 78/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9029 - acc: 0.6602Epoch 00077: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.9029 - acc: 0.6602 - val_loss: 1.0262 - val_acc: 0.6158\n",
      "Epoch 79/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8962 - acc: 0.6636Epoch 00078: val_loss improved from 1.01943 to 1.01686, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 0.8962 - acc: 0.6636 - val_loss: 1.0169 - val_acc: 0.6258\n",
      "Epoch 80/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8893 - acc: 0.6683Epoch 00079: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8891 - acc: 0.6683 - val_loss: 1.0326 - val_acc: 0.6225\n",
      "Epoch 81/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8948 - acc: 0.6668Epoch 00080: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8945 - acc: 0.6668 - val_loss: 1.0254 - val_acc: 0.6252\n",
      "Epoch 82/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8875 - acc: 0.6671Epoch 00081: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8874 - acc: 0.6671 - val_loss: 1.0214 - val_acc: 0.6266\n",
      "Epoch 83/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8783 - acc: 0.6701Epoch 00082: val_loss improved from 1.01686 to 1.01154, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 0.8785 - acc: 0.6701 - val_loss: 1.0115 - val_acc: 0.6297\n",
      "Epoch 84/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8820 - acc: 0.6693Epoch 00083: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8819 - acc: 0.6693 - val_loss: 1.0236 - val_acc: 0.6230\n",
      "Epoch 85/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8807 - acc: 0.6679Epoch 00084: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8808 - acc: 0.6678 - val_loss: 1.0372 - val_acc: 0.6236\n",
      "Epoch 86/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8714 - acc: 0.6744Epoch 00085: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8714 - acc: 0.6745 - val_loss: 1.0527 - val_acc: 0.6205\n",
      "Epoch 87/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8658 - acc: 0.6745Epoch 00086: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 0.8661 - acc: 0.6744 - val_loss: 1.0435 - val_acc: 0.6211\n",
      "Epoch 88/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8688 - acc: 0.6764Epoch 00087: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8688 - acc: 0.6764 - val_loss: 1.0688 - val_acc: 0.6141\n",
      "Epoch 89/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8583 - acc: 0.6792Epoch 00088: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8579 - acc: 0.6794 - val_loss: 1.0345 - val_acc: 0.6283\n",
      "Epoch 90/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8615 - acc: 0.6792Epoch 00089: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8616 - acc: 0.6792 - val_loss: 1.0596 - val_acc: 0.6174\n",
      "Epoch 91/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8606 - acc: 0.6781Epoch 00090: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8609 - acc: 0.6780 - val_loss: 1.0164 - val_acc: 0.6269\n",
      "Epoch 92/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8575 - acc: 0.6819Epoch 00091: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8578 - acc: 0.6818 - val_loss: 1.0865 - val_acc: 0.6069\n",
      "Epoch 93/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8519 - acc: 0.6832Epoch 00092: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8515 - acc: 0.6833 - val_loss: 1.0146 - val_acc: 0.6230\n",
      "Epoch 94/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8485 - acc: 0.6845Epoch 00093: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8485 - acc: 0.6845 - val_loss: 1.0304 - val_acc: 0.6219\n",
      "Epoch 95/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8452 - acc: 0.6831Epoch 00094: val_loss improved from 1.01154 to 1.00717, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 69s - loss: 0.8454 - acc: 0.6831 - val_loss: 1.0072 - val_acc: 0.6286\n",
      "Epoch 96/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8444 - acc: 0.6836Epoch 00095: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8443 - acc: 0.6837 - val_loss: 1.0305 - val_acc: 0.6208\n",
      "Epoch 97/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8422 - acc: 0.6881Epoch 00096: val_loss improved from 1.00717 to 1.00538, saving model to face_model1.h5\n",
      "28709/28709 [==============================] - 68s - loss: 0.8422 - acc: 0.6881 - val_loss: 1.0054 - val_acc: 0.6330\n",
      "Epoch 98/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8393 - acc: 0.6844Epoch 00097: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8392 - acc: 0.6845 - val_loss: 1.0362 - val_acc: 0.6322\n",
      "Epoch 99/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8313 - acc: 0.6902Epoch 00098: val_loss did not improve\n",
      "28709/28709 [==============================] - 69s - loss: 0.8314 - acc: 0.6902 - val_loss: 1.0626 - val_acc: 0.6199\n",
      "Epoch 100/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8300 - acc: 0.6904Epoch 00099: val_loss did not improve\n",
      "28709/28709 [==============================] - 68s - loss: 0.8298 - acc: 0.6904 - val_loss: 1.0175 - val_acc: 0.6381\n"
     ]
    }
   ],
   "source": [
    "print(\"Model no :\", 1)\n",
    "cur_model = model_list[0].fit(X_train, y_train,\n",
    "                    batch_size=batch_size,epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer[0]])\n",
    "train_model.append(cur_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model no : 2\n",
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.9997 - acc: 0.2124Epoch 00000: val_loss improved from inf to 1.81171, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 34s - loss: 1.9995 - acc: 0.2124 - val_loss: 1.8117 - val_acc: 0.2494\n",
      "Epoch 2/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.8329 - acc: 0.2472Epoch 00001: val_loss improved from 1.81171 to 1.80076, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.8328 - acc: 0.2472 - val_loss: 1.8008 - val_acc: 0.2494\n",
      "Epoch 3/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.8182 - acc: 0.2500Epoch 00002: val_loss improved from 1.80076 to 1.79410, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.8185 - acc: 0.2498 - val_loss: 1.7941 - val_acc: 0.2491\n",
      "Epoch 4/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.8056 - acc: 0.2530Epoch 00003: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.8057 - acc: 0.2530 - val_loss: 1.8069 - val_acc: 0.2491\n",
      "Epoch 5/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7883 - acc: 0.2591Epoch 00004: val_loss improved from 1.79410 to 1.74714, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.7883 - acc: 0.2589 - val_loss: 1.7471 - val_acc: 0.2767\n",
      "Epoch 6/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7573 - acc: 0.2776Epoch 00005: val_loss improved from 1.74714 to 1.69704, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.7572 - acc: 0.2776 - val_loss: 1.6970 - val_acc: 0.2990\n",
      "Epoch 7/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7286 - acc: 0.2945Epoch 00006: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.7286 - acc: 0.2945 - val_loss: 1.7197 - val_acc: 0.2956\n",
      "Epoch 8/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6975 - acc: 0.3090Epoch 00007: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.6977 - acc: 0.3090 - val_loss: 1.8395 - val_acc: 0.2831\n",
      "Epoch 9/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6559 - acc: 0.3369Epoch 00008: val_loss improved from 1.69704 to 1.58648, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.6559 - acc: 0.3370 - val_loss: 1.5865 - val_acc: 0.3670\n",
      "Epoch 10/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6268 - acc: 0.3561Epoch 00009: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.6268 - acc: 0.3561 - val_loss: 1.7893 - val_acc: 0.2903\n",
      "Epoch 11/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6038 - acc: 0.3654Epoch 00010: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.6036 - acc: 0.3655 - val_loss: 2.0989 - val_acc: 0.2973\n",
      "Epoch 12/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5745 - acc: 0.3854Epoch 00011: val_loss improved from 1.58648 to 1.48572, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.5749 - acc: 0.3852 - val_loss: 1.4857 - val_acc: 0.4093\n",
      "Epoch 13/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5518 - acc: 0.3936Epoch 00012: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.5520 - acc: 0.3935 - val_loss: 1.5049 - val_acc: 0.4191\n",
      "Epoch 14/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5277 - acc: 0.4058Epoch 00013: val_loss improved from 1.48572 to 1.40798, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 19s - loss: 1.5277 - acc: 0.4059 - val_loss: 1.4080 - val_acc: 0.4505\n",
      "Epoch 15/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4994 - acc: 0.4176Epoch 00014: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4993 - acc: 0.4176 - val_loss: 1.4407 - val_acc: 0.4333\n",
      "Epoch 16/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4819 - acc: 0.4301Epoch 00015: val_loss improved from 1.40798 to 1.38694, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 19s - loss: 1.4819 - acc: 0.4300 - val_loss: 1.3869 - val_acc: 0.4444\n",
      "Epoch 17/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4672 - acc: 0.4353Epoch 00016: val_loss improved from 1.38694 to 1.36356, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 19s - loss: 1.4671 - acc: 0.4354 - val_loss: 1.3636 - val_acc: 0.4698\n",
      "Epoch 18/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4569 - acc: 0.4428Epoch 00017: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4568 - acc: 0.4428 - val_loss: 1.4759 - val_acc: 0.4361\n",
      "Epoch 19/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4389 - acc: 0.4424Epoch 00018: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4388 - acc: 0.4425 - val_loss: 1.4206 - val_acc: 0.4497\n",
      "Epoch 20/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4289 - acc: 0.4527Epoch 00019: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4291 - acc: 0.4527 - val_loss: 1.3908 - val_acc: 0.4542\n",
      "Epoch 21/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4137 - acc: 0.4566Epoch 00020: val_loss improved from 1.36356 to 1.31210, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 19s - loss: 1.4137 - acc: 0.4566 - val_loss: 1.3121 - val_acc: 0.4946\n",
      "Epoch 22/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4033 - acc: 0.4633Epoch 00021: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4032 - acc: 0.4634 - val_loss: 1.3654 - val_acc: 0.4670\n",
      "Epoch 23/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3969 - acc: 0.4640Epoch 00022: val_loss improved from 1.31210 to 1.30243, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.3968 - acc: 0.4642 - val_loss: 1.3024 - val_acc: 0.4948\n",
      "Epoch 24/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3829 - acc: 0.4701Epoch 00023: val_loss improved from 1.30243 to 1.29128, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 19s - loss: 1.3831 - acc: 0.4701 - val_loss: 1.2913 - val_acc: 0.4999\n",
      "Epoch 25/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3723 - acc: 0.4746Epoch 00024: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3720 - acc: 0.4747 - val_loss: 1.5257 - val_acc: 0.4388\n",
      "Epoch 26/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3645 - acc: 0.4771Epoch 00025: val_loss improved from 1.29128 to 1.26019, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.3643 - acc: 0.4772 - val_loss: 1.2602 - val_acc: 0.5124\n",
      "Epoch 27/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3523 - acc: 0.4833Epoch 00026: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3520 - acc: 0.4835 - val_loss: 1.2640 - val_acc: 0.5079\n",
      "Epoch 28/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3526 - acc: 0.4857Epoch 00027: val_loss improved from 1.26019 to 1.23445, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.3520 - acc: 0.4859 - val_loss: 1.2344 - val_acc: 0.5255\n",
      "Epoch 29/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3420 - acc: 0.4892Epoch 00028: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3419 - acc: 0.4892 - val_loss: 1.2836 - val_acc: 0.5107\n",
      "Epoch 30/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3356 - acc: 0.4941Epoch 00029: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709/28709 [==============================] - 18s - loss: 1.3355 - acc: 0.4941 - val_loss: 1.4084 - val_acc: 0.4843\n",
      "Epoch 31/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3305 - acc: 0.4935Epoch 00030: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3309 - acc: 0.4933 - val_loss: 1.2386 - val_acc: 0.5174\n",
      "Epoch 32/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3261 - acc: 0.4977Epoch 00031: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3265 - acc: 0.4976 - val_loss: 1.2686 - val_acc: 0.5113\n",
      "Epoch 33/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3212 - acc: 0.4980Epoch 00032: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3210 - acc: 0.4979 - val_loss: 1.2574 - val_acc: 0.5082\n",
      "Epoch 34/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3115 - acc: 0.5022Epoch 00033: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3114 - acc: 0.5021 - val_loss: 1.3248 - val_acc: 0.4812\n",
      "Epoch 35/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3105 - acc: 0.5031Epoch 00034: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3107 - acc: 0.5030 - val_loss: 1.3052 - val_acc: 0.4829\n",
      "Epoch 36/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3013 - acc: 0.5057Epoch 00035: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3009 - acc: 0.5058 - val_loss: 1.2915 - val_acc: 0.5054\n",
      "Epoch 37/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2978 - acc: 0.5095Epoch 00036: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2976 - acc: 0.5096 - val_loss: 1.2565 - val_acc: 0.5099\n",
      "Epoch 38/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2929 - acc: 0.5099Epoch 00037: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2930 - acc: 0.5097 - val_loss: 1.2572 - val_acc: 0.5127\n",
      "Epoch 39/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2877 - acc: 0.5105Epoch 00038: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2878 - acc: 0.5105 - val_loss: 1.2390 - val_acc: 0.5288\n",
      "Epoch 40/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2879 - acc: 0.5127Epoch 00039: val_loss improved from 1.23445 to 1.21309, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2879 - acc: 0.5127 - val_loss: 1.2131 - val_acc: 0.5333\n",
      "Epoch 41/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2785 - acc: 0.5179Epoch 00040: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2782 - acc: 0.5180 - val_loss: 1.2499 - val_acc: 0.5258\n",
      "Epoch 42/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2755 - acc: 0.5196Epoch 00041: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2752 - acc: 0.5198 - val_loss: 1.2743 - val_acc: 0.5052\n",
      "Epoch 43/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2761 - acc: 0.5196Epoch 00042: val_loss improved from 1.21309 to 1.21292, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2757 - acc: 0.5199 - val_loss: 1.2129 - val_acc: 0.5339\n",
      "Epoch 44/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2729 - acc: 0.5194Epoch 00043: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2728 - acc: 0.5194 - val_loss: 1.2599 - val_acc: 0.5177\n",
      "Epoch 45/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2673 - acc: 0.5230Epoch 00044: val_loss improved from 1.21292 to 1.18531, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2672 - acc: 0.5230 - val_loss: 1.1853 - val_acc: 0.5411\n",
      "Epoch 46/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2640 - acc: 0.5211Epoch 00045: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2641 - acc: 0.5211 - val_loss: 1.2798 - val_acc: 0.5205\n",
      "Epoch 47/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2624 - acc: 0.5266Epoch 00046: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2627 - acc: 0.5265 - val_loss: 1.2159 - val_acc: 0.5258\n",
      "Epoch 48/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2588 - acc: 0.5238Epoch 00047: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2587 - acc: 0.5238 - val_loss: 1.1986 - val_acc: 0.5400\n",
      "Epoch 49/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2565 - acc: 0.5253Epoch 00048: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2567 - acc: 0.5251 - val_loss: 1.1881 - val_acc: 0.5425\n",
      "Epoch 50/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2491 - acc: 0.5264Epoch 00049: val_loss improved from 1.18531 to 1.18225, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2490 - acc: 0.5264 - val_loss: 1.1823 - val_acc: 0.5467\n",
      "Epoch 51/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2496 - acc: 0.5303Epoch 00050: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2495 - acc: 0.5304 - val_loss: 1.2144 - val_acc: 0.5325\n",
      "Epoch 52/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2416 - acc: 0.5311Epoch 00051: val_loss improved from 1.18225 to 1.17898, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2419 - acc: 0.5310 - val_loss: 1.1790 - val_acc: 0.5514\n",
      "Epoch 53/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2398 - acc: 0.5344Epoch 00052: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2398 - acc: 0.5344 - val_loss: 1.2162 - val_acc: 0.5383\n",
      "Epoch 54/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2386 - acc: 0.5353Epoch 00053: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2388 - acc: 0.5351 - val_loss: 1.2231 - val_acc: 0.5380\n",
      "Epoch 55/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2346 - acc: 0.5365Epoch 00054: val_loss improved from 1.17898 to 1.16245, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2346 - acc: 0.5364 - val_loss: 1.1625 - val_acc: 0.5542\n",
      "Epoch 56/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2379 - acc: 0.5339Epoch 00055: val_loss improved from 1.16245 to 1.15847, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2378 - acc: 0.5340 - val_loss: 1.1585 - val_acc: 0.5567\n",
      "Epoch 57/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2386 - acc: 0.5329Epoch 00056: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2389 - acc: 0.5328 - val_loss: 1.3524 - val_acc: 0.4865\n",
      "Epoch 58/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2264 - acc: 0.5386Epoch 00057: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2264 - acc: 0.5385 - val_loss: 1.1679 - val_acc: 0.5578\n",
      "Epoch 59/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2265 - acc: 0.5361Epoch 00058: val_loss improved from 1.15847 to 1.15816, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2266 - acc: 0.5361 - val_loss: 1.1582 - val_acc: 0.5589\n",
      "Epoch 60/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2172 - acc: 0.5405Epoch 00059: val_loss improved from 1.15816 to 1.15360, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2175 - acc: 0.5403 - val_loss: 1.1536 - val_acc: 0.5620\n",
      "Epoch 61/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2168 - acc: 0.5429Epoch 00060: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709/28709 [==============================] - 18s - loss: 1.2168 - acc: 0.5430 - val_loss: 1.1557 - val_acc: 0.5570\n",
      "Epoch 62/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2241 - acc: 0.5384Epoch 00061: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2241 - acc: 0.5384 - val_loss: 1.1791 - val_acc: 0.5495\n",
      "Epoch 63/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2206 - acc: 0.5380Epoch 00062: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2202 - acc: 0.5382 - val_loss: 1.1740 - val_acc: 0.5536\n",
      "Epoch 64/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2144 - acc: 0.5428Epoch 00063: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2144 - acc: 0.5428 - val_loss: 1.1719 - val_acc: 0.5478\n",
      "Epoch 65/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2177 - acc: 0.5426Epoch 00064: val_loss improved from 1.15360 to 1.14602, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2179 - acc: 0.5427 - val_loss: 1.1460 - val_acc: 0.5592\n",
      "Epoch 66/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2091 - acc: 0.5441Epoch 00065: val_loss improved from 1.14602 to 1.11979, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2090 - acc: 0.5443 - val_loss: 1.1198 - val_acc: 0.5687\n",
      "Epoch 67/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2105 - acc: 0.5426Epoch 00066: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2109 - acc: 0.5424 - val_loss: 1.1629 - val_acc: 0.5589\n",
      "Epoch 68/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2081 - acc: 0.5442Epoch 00067: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2079 - acc: 0.5442 - val_loss: 1.1307 - val_acc: 0.5723\n",
      "Epoch 69/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2105 - acc: 0.5442Epoch 00068: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2109 - acc: 0.5440 - val_loss: 1.1204 - val_acc: 0.5770\n",
      "Epoch 70/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2057 - acc: 0.5459Epoch 00069: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2063 - acc: 0.5456 - val_loss: 1.1403 - val_acc: 0.5631\n",
      "Epoch 71/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2072 - acc: 0.5430Epoch 00070: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2072 - acc: 0.5429 - val_loss: 1.1655 - val_acc: 0.5575\n",
      "Epoch 72/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2099 - acc: 0.5422Epoch 00071: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2096 - acc: 0.5425 - val_loss: 1.1275 - val_acc: 0.5687\n",
      "Epoch 73/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2055 - acc: 0.5465Epoch 00072: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2055 - acc: 0.5464 - val_loss: 1.1465 - val_acc: 0.5656\n",
      "Epoch 74/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1993 - acc: 0.5475Epoch 00073: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1991 - acc: 0.5476 - val_loss: 1.1465 - val_acc: 0.5651\n",
      "Epoch 75/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1922 - acc: 0.5536Epoch 00074: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1920 - acc: 0.5539 - val_loss: 1.1305 - val_acc: 0.5681\n",
      "Epoch 76/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1976 - acc: 0.5495Epoch 00075: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1976 - acc: 0.5495 - val_loss: 1.2097 - val_acc: 0.5433\n",
      "Epoch 77/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1983 - acc: 0.5500Epoch 00076: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1985 - acc: 0.5500 - val_loss: 1.1677 - val_acc: 0.5556\n",
      "Epoch 78/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1955 - acc: 0.5520Epoch 00077: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1953 - acc: 0.5522 - val_loss: 1.1309 - val_acc: 0.5612\n",
      "Epoch 79/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2046 - acc: 0.5508Epoch 00078: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2047 - acc: 0.5508 - val_loss: 1.1222 - val_acc: 0.5729\n",
      "Epoch 80/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1900 - acc: 0.5507Epoch 00079: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1899 - acc: 0.5508 - val_loss: 1.1505 - val_acc: 0.5670\n",
      "Epoch 81/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1970 - acc: 0.5517Epoch 00080: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1975 - acc: 0.5517 - val_loss: 1.1514 - val_acc: 0.5553\n",
      "Epoch 82/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1898 - acc: 0.5518Epoch 00081: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1900 - acc: 0.5517 - val_loss: 1.2803 - val_acc: 0.5130\n",
      "Epoch 83/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1892 - acc: 0.5534Epoch 00082: val_loss improved from 1.11979 to 1.11662, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.1890 - acc: 0.5535 - val_loss: 1.1166 - val_acc: 0.5832\n",
      "Epoch 84/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1888 - acc: 0.5523Epoch 00083: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1884 - acc: 0.5523 - val_loss: 1.1171 - val_acc: 0.5743\n",
      "Epoch 85/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1901 - acc: 0.5494Epoch 00084: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1900 - acc: 0.5495 - val_loss: 1.1677 - val_acc: 0.5626\n",
      "Epoch 86/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1836 - acc: 0.5571Epoch 00085: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1833 - acc: 0.5572 - val_loss: 1.1239 - val_acc: 0.5665\n",
      "Epoch 87/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1890 - acc: 0.5548Epoch 00086: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1892 - acc: 0.5548 - val_loss: 1.2139 - val_acc: 0.5344\n",
      "Epoch 88/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1798 - acc: 0.5561Epoch 00087: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1798 - acc: 0.5560 - val_loss: 1.1373 - val_acc: 0.5637\n",
      "Epoch 89/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1923 - acc: 0.5494Epoch 00088: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1921 - acc: 0.5495 - val_loss: 1.1236 - val_acc: 0.5720\n",
      "Epoch 90/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1843 - acc: 0.5531Epoch 00089: val_loss improved from 1.11662 to 1.10510, saving model to face_model2.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.1845 - acc: 0.5531 - val_loss: 1.1051 - val_acc: 0.5779\n",
      "Epoch 91/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1781 - acc: 0.5555Epoch 00090: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1782 - acc: 0.5554 - val_loss: 1.1189 - val_acc: 0.5740\n",
      "Epoch 92/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1845 - acc: 0.5556Epoch 00091: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1842 - acc: 0.5556 - val_loss: 1.1232 - val_acc: 0.5779\n",
      "Epoch 93/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1835 - acc: 0.5570Epoch 00092: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709/28709 [==============================] - 18s - loss: 1.1834 - acc: 0.5570 - val_loss: 1.1376 - val_acc: 0.5756\n",
      "Epoch 94/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1801 - acc: 0.5548Epoch 00093: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1802 - acc: 0.5547 - val_loss: 1.1110 - val_acc: 0.5704\n",
      "Epoch 95/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1826 - acc: 0.5547Epoch 00094: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1822 - acc: 0.5548 - val_loss: 1.1141 - val_acc: 0.5712\n",
      "Epoch 96/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1816 - acc: 0.5580Epoch 00095: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1817 - acc: 0.5579 - val_loss: 1.1640 - val_acc: 0.5520\n",
      "Epoch 97/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1780 - acc: 0.5557Epoch 00096: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1782 - acc: 0.5555 - val_loss: 1.1533 - val_acc: 0.5631\n",
      "Epoch 98/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1758 - acc: 0.5564Epoch 00097: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1757 - acc: 0.5564 - val_loss: 1.1183 - val_acc: 0.5731\n",
      "Epoch 99/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1787 - acc: 0.5576Epoch 00098: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1786 - acc: 0.5576 - val_loss: 1.1396 - val_acc: 0.5676\n",
      "Epoch 100/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1715 - acc: 0.5588Epoch 00099: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1716 - acc: 0.5586 - val_loss: 1.1628 - val_acc: 0.5542\n"
     ]
    }
   ],
   "source": [
    "print(\"Model no :\", 2)\n",
    "cur_model = model_list[1].fit(X_train, y_train,\n",
    "                    batch_size=batch_size,epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer[1]])\n",
    "train_model.append(cur_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model no : 3\n",
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.9992 - acc: 0.2114Epoch 00000: val_loss improved from inf to 1.82055, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 61s - loss: 1.9990 - acc: 0.2113 - val_loss: 1.8205 - val_acc: 0.2494\n",
      "Epoch 2/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.8375 - acc: 0.2458Epoch 00001: val_loss improved from 1.82055 to 1.80900, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.8375 - acc: 0.2458 - val_loss: 1.8090 - val_acc: 0.2494\n",
      "Epoch 3/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.8158 - acc: 0.2534Epoch 00002: val_loss improved from 1.80900 to 1.79884, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.8157 - acc: 0.2535 - val_loss: 1.7988 - val_acc: 0.2508\n",
      "Epoch 4/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7698 - acc: 0.2759Epoch 00003: val_loss improved from 1.79884 to 1.74073, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.7699 - acc: 0.2758 - val_loss: 1.7407 - val_acc: 0.2817\n",
      "Epoch 5/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7154 - acc: 0.2969Epoch 00004: val_loss improved from 1.74073 to 1.68597, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.7154 - acc: 0.2968 - val_loss: 1.6860 - val_acc: 0.2959\n",
      "Epoch 6/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6630 - acc: 0.3169Epoch 00005: val_loss improved from 1.68597 to 1.57004, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.6630 - acc: 0.3169 - val_loss: 1.5700 - val_acc: 0.3742\n",
      "Epoch 7/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6188 - acc: 0.3491Epoch 00006: val_loss improved from 1.57004 to 1.51523, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.6187 - acc: 0.3490 - val_loss: 1.5152 - val_acc: 0.4124\n",
      "Epoch 8/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5808 - acc: 0.3758Epoch 00007: val_loss improved from 1.51523 to 1.45744, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.5812 - acc: 0.3758 - val_loss: 1.4574 - val_acc: 0.4079\n",
      "Epoch 9/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5450 - acc: 0.3831Epoch 00008: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.5450 - acc: 0.3831 - val_loss: 1.4824 - val_acc: 0.4107\n",
      "Epoch 10/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5095 - acc: 0.4009Epoch 00009: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.5095 - acc: 0.4009 - val_loss: 1.4830 - val_acc: 0.4079\n",
      "Epoch 11/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4894 - acc: 0.4079Epoch 00010: val_loss improved from 1.45744 to 1.38619, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.4891 - acc: 0.4080 - val_loss: 1.3862 - val_acc: 0.4675\n",
      "Epoch 12/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4673 - acc: 0.4279Epoch 00011: val_loss improved from 1.38619 to 1.37859, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.4673 - acc: 0.4280 - val_loss: 1.3786 - val_acc: 0.4492\n",
      "Epoch 13/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4308 - acc: 0.4421Epoch 00012: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.4307 - acc: 0.4421 - val_loss: 1.4206 - val_acc: 0.4427\n",
      "Epoch 14/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4195 - acc: 0.4503Epoch 00013: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.4194 - acc: 0.4503 - val_loss: 1.4864 - val_acc: 0.4110\n",
      "Epoch 15/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4100 - acc: 0.4556Epoch 00014: val_loss improved from 1.37859 to 1.36423, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.4105 - acc: 0.4555 - val_loss: 1.3642 - val_acc: 0.4547\n",
      "Epoch 16/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3788 - acc: 0.4721Epoch 00015: val_loss improved from 1.36423 to 1.27311, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.3789 - acc: 0.4721 - val_loss: 1.2731 - val_acc: 0.5160\n",
      "Epoch 17/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3605 - acc: 0.4826Epoch 00016: val_loss improved from 1.27311 to 1.25983, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.3602 - acc: 0.4826 - val_loss: 1.2598 - val_acc: 0.5079\n",
      "Epoch 18/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3509 - acc: 0.4864Epoch 00017: val_loss improved from 1.25983 to 1.25579, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.3511 - acc: 0.4864 - val_loss: 1.2558 - val_acc: 0.5177\n",
      "Epoch 19/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3320 - acc: 0.4897Epoch 00018: val_loss improved from 1.25579 to 1.24096, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.3321 - acc: 0.4896 - val_loss: 1.2410 - val_acc: 0.5252\n",
      "Epoch 20/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3160 - acc: 0.4990Epoch 00019: val_loss improved from 1.24096 to 1.22866, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.3161 - acc: 0.4988 - val_loss: 1.2287 - val_acc: 0.5319\n",
      "Epoch 21/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3080 - acc: 0.5007Epoch 00020: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.3082 - acc: 0.5007 - val_loss: 1.3313 - val_acc: 0.4714\n",
      "Epoch 22/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2909 - acc: 0.5073Epoch 00021: val_loss improved from 1.22866 to 1.21085, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.2905 - acc: 0.5074 - val_loss: 1.2108 - val_acc: 0.5325\n",
      "Epoch 23/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2739 - acc: 0.5151Epoch 00022: val_loss improved from 1.21085 to 1.20088, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.2744 - acc: 0.5152 - val_loss: 1.2009 - val_acc: 0.5478\n",
      "Epoch 24/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2682 - acc: 0.5178Epoch 00023: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.2682 - acc: 0.5177 - val_loss: 1.2399 - val_acc: 0.5227\n",
      "Epoch 25/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2523 - acc: 0.5247Epoch 00024: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.2524 - acc: 0.5247 - val_loss: 1.2255 - val_acc: 0.5316\n",
      "Epoch 26/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2300 - acc: 0.5304Epoch 00025: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.2299 - acc: 0.5304 - val_loss: 1.2041 - val_acc: 0.5531\n",
      "Epoch 27/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2335 - acc: 0.5359Epoch 00026: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.2333 - acc: 0.5359 - val_loss: 1.2038 - val_acc: 0.5403\n",
      "Epoch 28/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2159 - acc: 0.5402Epoch 00027: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.2162 - acc: 0.5401 - val_loss: 1.2182 - val_acc: 0.5210\n",
      "Epoch 29/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2102 - acc: 0.5421Epoch 00028: val_loss improved from 1.20088 to 1.19479, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.2100 - acc: 0.5422 - val_loss: 1.1948 - val_acc: 0.5458\n",
      "Epoch 30/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1974 - acc: 0.5508Epoch 00029: val_loss improved from 1.19479 to 1.17297, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.1973 - acc: 0.5508 - val_loss: 1.1730 - val_acc: 0.5559\n",
      "Epoch 31/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1784 - acc: 0.5532Epoch 00030: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.1783 - acc: 0.5532 - val_loss: 1.2042 - val_acc: 0.5542\n",
      "Epoch 32/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1755 - acc: 0.5620Epoch 00031: val_loss improved from 1.17297 to 1.16862, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.1758 - acc: 0.5618 - val_loss: 1.1686 - val_acc: 0.5645\n",
      "Epoch 33/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1554 - acc: 0.5670Epoch 00032: val_loss improved from 1.16862 to 1.14812, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.1558 - acc: 0.5669 - val_loss: 1.1481 - val_acc: 0.5670\n",
      "Epoch 34/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1456 - acc: 0.5777Epoch 00033: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.1452 - acc: 0.5779 - val_loss: 1.1676 - val_acc: 0.5784\n",
      "Epoch 35/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1261 - acc: 0.5795Epoch 00034: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.1265 - acc: 0.5794 - val_loss: 1.1711 - val_acc: 0.5614\n",
      "Epoch 36/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1153 - acc: 0.5895Epoch 00035: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.1152 - acc: 0.5895 - val_loss: 1.1818 - val_acc: 0.5631\n",
      "Epoch 37/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1034 - acc: 0.5936Epoch 00036: val_loss improved from 1.14812 to 1.13166, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 1.1030 - acc: 0.5936 - val_loss: 1.1317 - val_acc: 0.5795\n",
      "Epoch 38/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0872 - acc: 0.5975Epoch 00037: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.0873 - acc: 0.5974 - val_loss: 1.1338 - val_acc: 0.5871\n",
      "Epoch 39/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0742 - acc: 0.6024Epoch 00038: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.0745 - acc: 0.6023 - val_loss: 1.1463 - val_acc: 0.5926\n",
      "Epoch 40/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0644 - acc: 0.6078Epoch 00039: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.0645 - acc: 0.6076 - val_loss: 1.1801 - val_acc: 0.5815\n",
      "Epoch 41/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0551 - acc: 0.6111Epoch 00040: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.0552 - acc: 0.6111 - val_loss: 1.1329 - val_acc: 0.5840\n",
      "Epoch 42/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0377 - acc: 0.6208Epoch 00041: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.0375 - acc: 0.6207 - val_loss: 1.1648 - val_acc: 0.5776\n",
      "Epoch 43/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0239 - acc: 0.6233Epoch 00042: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.0236 - acc: 0.6233 - val_loss: 1.1551 - val_acc: 0.5756\n",
      "Epoch 44/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0096 - acc: 0.6351Epoch 00043: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 1.0096 - acc: 0.6350 - val_loss: 1.1396 - val_acc: 0.5954\n",
      "Epoch 45/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9935 - acc: 0.6360Epoch 00044: val_loss improved from 1.13166 to 1.12278, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 0.9935 - acc: 0.6359 - val_loss: 1.1228 - val_acc: 0.5910\n",
      "Epoch 46/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9903 - acc: 0.6365Epoch 00045: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.9905 - acc: 0.6365 - val_loss: 1.1489 - val_acc: 0.5821\n",
      "Epoch 47/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9784 - acc: 0.6414Epoch 00046: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.9787 - acc: 0.6413 - val_loss: 1.1523 - val_acc: 0.5874\n",
      "Epoch 48/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9558 - acc: 0.6498Epoch 00047: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.9562 - acc: 0.6497 - val_loss: 1.1424 - val_acc: 0.6002\n",
      "Epoch 49/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9502 - acc: 0.6538Epoch 00048: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.9505 - acc: 0.6536 - val_loss: 1.2045 - val_acc: 0.5751\n",
      "Epoch 50/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9333 - acc: 0.6635Epoch 00049: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.9333 - acc: 0.6635 - val_loss: 1.1891 - val_acc: 0.5957\n",
      "Epoch 51/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9235 - acc: 0.6643Epoch 00050: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.9234 - acc: 0.6642 - val_loss: 1.1820 - val_acc: 0.5988\n",
      "Epoch 52/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9108 - acc: 0.6702Epoch 00051: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.9109 - acc: 0.6703 - val_loss: 1.1863 - val_acc: 0.6069\n",
      "Epoch 53/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8921 - acc: 0.6771Epoch 00052: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.8918 - acc: 0.6772 - val_loss: 1.1678 - val_acc: 0.6030\n",
      "Epoch 54/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9607 - acc: 0.6535Epoch 00053: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.9608 - acc: 0.6535 - val_loss: 1.1343 - val_acc: 0.6016\n",
      "Epoch 55/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8973 - acc: 0.6801Epoch 00054: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.8971 - acc: 0.6801 - val_loss: 1.1937 - val_acc: 0.5848\n",
      "Epoch 56/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8729 - acc: 0.6836Epoch 00055: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.8728 - acc: 0.6836 - val_loss: 1.1772 - val_acc: 0.5915\n",
      "Epoch 57/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8685 - acc: 0.6865Epoch 00056: val_loss improved from 1.12278 to 1.12192, saving model to face_model3.h5\n",
      "28709/28709 [==============================] - 42s - loss: 0.8684 - acc: 0.6865 - val_loss: 1.1219 - val_acc: 0.6071\n",
      "Epoch 58/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8412 - acc: 0.6982Epoch 00057: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.8409 - acc: 0.6983 - val_loss: 1.1818 - val_acc: 0.6016\n",
      "Epoch 59/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8338 - acc: 0.6976Epoch 00058: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.8336 - acc: 0.6977 - val_loss: 1.1923 - val_acc: 0.5996\n",
      "Epoch 60/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8222 - acc: 0.7071Epoch 00059: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.8223 - acc: 0.7071 - val_loss: 1.2047 - val_acc: 0.5999\n",
      "Epoch 61/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8062 - acc: 0.7119Epoch 00060: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.8064 - acc: 0.7119 - val_loss: 1.1820 - val_acc: 0.6116\n",
      "Epoch 62/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8036 - acc: 0.7143Epoch 00061: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.8038 - acc: 0.7141 - val_loss: 1.2092 - val_acc: 0.6077\n",
      "Epoch 63/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7894 - acc: 0.7180Epoch 00062: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.7892 - acc: 0.7181 - val_loss: 1.2281 - val_acc: 0.6080\n",
      "Epoch 64/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7762 - acc: 0.7224Epoch 00063: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.7768 - acc: 0.7221 - val_loss: 1.2285 - val_acc: 0.6119\n",
      "Epoch 65/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7675 - acc: 0.7264Epoch 00064: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.7680 - acc: 0.7262 - val_loss: 1.2514 - val_acc: 0.6121\n",
      "Epoch 66/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7643 - acc: 0.7320Epoch 00065: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.7642 - acc: 0.7320 - val_loss: 1.2826 - val_acc: 0.6057\n",
      "Epoch 67/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7613 - acc: 0.7304Epoch 00066: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.7617 - acc: 0.7303 - val_loss: 1.2277 - val_acc: 0.6091\n",
      "Epoch 68/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7429 - acc: 0.7370Epoch 00067: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.7437 - acc: 0.7369 - val_loss: 1.2651 - val_acc: 0.6094\n",
      "Epoch 69/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7341 - acc: 0.7422Epoch 00068: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.7341 - acc: 0.7422 - val_loss: 1.2247 - val_acc: 0.6102\n",
      "Epoch 70/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7263 - acc: 0.7474Epoch 00069: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.7264 - acc: 0.7473 - val_loss: 1.2646 - val_acc: 0.6219\n",
      "Epoch 71/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7333 - acc: 0.7441Epoch 00070: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.7331 - acc: 0.7442 - val_loss: 1.2260 - val_acc: 0.6035\n",
      "Epoch 72/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6998 - acc: 0.7532Epoch 00071: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6998 - acc: 0.7533 - val_loss: 1.2345 - val_acc: 0.6060\n",
      "Epoch 73/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6861 - acc: 0.7570Epoch 00072: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6858 - acc: 0.7571 - val_loss: 1.3175 - val_acc: 0.6069\n",
      "Epoch 74/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6886 - acc: 0.7569Epoch 00073: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6890 - acc: 0.7568 - val_loss: 1.2995 - val_acc: 0.6244\n",
      "Epoch 75/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6851 - acc: 0.7603Epoch 00074: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6849 - acc: 0.7604 - val_loss: 1.2691 - val_acc: 0.6119\n",
      "Epoch 76/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6585 - acc: 0.7677Epoch 00075: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6584 - acc: 0.7677 - val_loss: 1.3319 - val_acc: 0.6222\n",
      "Epoch 77/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6561 - acc: 0.7721Epoch 00076: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6561 - acc: 0.7721 - val_loss: 1.2813 - val_acc: 0.6135\n",
      "Epoch 78/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6540 - acc: 0.7712Epoch 00077: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6540 - acc: 0.7712 - val_loss: 1.3202 - val_acc: 0.6102\n",
      "Epoch 79/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6600 - acc: 0.7731Epoch 00078: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6602 - acc: 0.7730 - val_loss: 1.3609 - val_acc: 0.5812\n",
      "Epoch 80/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6232 - acc: 0.7814Epoch 00079: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6234 - acc: 0.7814 - val_loss: 1.2991 - val_acc: 0.6191\n",
      "Epoch 81/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6256 - acc: 0.7847Epoch 00080: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6254 - acc: 0.7847 - val_loss: 1.3695 - val_acc: 0.6060\n",
      "Epoch 82/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6147 - acc: 0.7870Epoch 00081: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6145 - acc: 0.7871 - val_loss: 1.2843 - val_acc: 0.6191\n",
      "Epoch 83/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6030 - acc: 0.7908Epoch 00082: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6030 - acc: 0.7908 - val_loss: 1.3634 - val_acc: 0.6188\n",
      "Epoch 84/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5995 - acc: 0.7965Epoch 00083: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.6003 - acc: 0.7962 - val_loss: 1.3225 - val_acc: 0.6225\n",
      "Epoch 85/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5932 - acc: 0.7968Epoch 00084: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5934 - acc: 0.7968 - val_loss: 1.3548 - val_acc: 0.6213\n",
      "Epoch 86/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5737 - acc: 0.8057Epoch 00085: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5734 - acc: 0.8058 - val_loss: 1.3936 - val_acc: 0.6166\n",
      "Epoch 87/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5834 - acc: 0.8045Epoch 00086: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5831 - acc: 0.8046 - val_loss: 1.3819 - val_acc: 0.5926\n",
      "Epoch 88/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5790 - acc: 0.8053Epoch 00087: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5788 - acc: 0.8055 - val_loss: 1.3522 - val_acc: 0.6172\n",
      "Epoch 89/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5617 - acc: 0.8086Epoch 00088: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5624 - acc: 0.8085 - val_loss: 1.4093 - val_acc: 0.6133\n",
      "Epoch 90/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5602 - acc: 0.8062Epoch 00089: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5603 - acc: 0.8062 - val_loss: 1.3866 - val_acc: 0.6105\n",
      "Epoch 91/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5499 - acc: 0.8133Epoch 00090: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5501 - acc: 0.8133 - val_loss: 1.3718 - val_acc: 0.6105\n",
      "Epoch 92/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8175Epoch 00091: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5366 - acc: 0.8175 - val_loss: 1.3802 - val_acc: 0.6227\n",
      "Epoch 93/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5393 - acc: 0.8181Epoch 00092: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5393 - acc: 0.8181 - val_loss: 1.4024 - val_acc: 0.6152\n",
      "Epoch 94/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5289 - acc: 0.8195Epoch 00093: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5291 - acc: 0.8194 - val_loss: 1.4468 - val_acc: 0.5952\n",
      "Epoch 95/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5143 - acc: 0.8260Epoch 00094: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5141 - acc: 0.8261 - val_loss: 1.4504 - val_acc: 0.6138\n",
      "Epoch 96/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8306Epoch 00095: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5077 - acc: 0.8305 - val_loss: 1.4495 - val_acc: 0.6227\n",
      "Epoch 97/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.8310Epoch 00096: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.5053 - acc: 0.8310 - val_loss: 1.4045 - val_acc: 0.6180\n",
      "Epoch 98/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.4968 - acc: 0.8357Epoch 00097: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.4970 - acc: 0.8355 - val_loss: 1.4313 - val_acc: 0.6241\n",
      "Epoch 99/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.8337Epoch 00098: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.4941 - acc: 0.8336 - val_loss: 1.4383 - val_acc: 0.6250\n",
      "Epoch 100/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.4808 - acc: 0.8408Epoch 00099: val_loss did not improve\n",
      "28709/28709 [==============================] - 42s - loss: 0.4810 - acc: 0.8407 - val_loss: 1.4664 - val_acc: 0.6244\n"
     ]
    }
   ],
   "source": [
    "print(\"Model no :\", 3)\n",
    "cur_model = model_list[2].fit(X_train, y_train,\n",
    "                    batch_size=batch_size,epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer[2]])\n",
    "train_model.append(cur_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model no : 4\n",
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 2.2603 - acc: 0.2127Epoch 00000: val_loss improved from inf to 1.82010, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 31s - loss: 2.2596 - acc: 0.2128 - val_loss: 1.8201 - val_acc: 0.2494\n",
      "Epoch 2/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.8432 - acc: 0.2491Epoch 00001: val_loss improved from 1.82010 to 1.78458, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.8432 - acc: 0.2492 - val_loss: 1.7846 - val_acc: 0.2558\n",
      "Epoch 3/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.8027 - acc: 0.2622Epoch 00002: val_loss improved from 1.78458 to 1.74324, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.8027 - acc: 0.2620 - val_loss: 1.7432 - val_acc: 0.2750\n",
      "Epoch 4/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7656 - acc: 0.2811Epoch 00003: val_loss improved from 1.74324 to 1.72210, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.7656 - acc: 0.2810 - val_loss: 1.7221 - val_acc: 0.2817\n",
      "Epoch 5/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7225 - acc: 0.3022Epoch 00004: val_loss improved from 1.72210 to 1.64313, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.7223 - acc: 0.3023 - val_loss: 1.6431 - val_acc: 0.3480\n",
      "Epoch 6/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6796 - acc: 0.3234Epoch 00005: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.6796 - acc: 0.3234 - val_loss: 1.6593 - val_acc: 0.3221\n",
      "Epoch 7/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6519 - acc: 0.3371Epoch 00006: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.6521 - acc: 0.3373 - val_loss: 1.6611 - val_acc: 0.3363\n",
      "Epoch 8/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6244 - acc: 0.3502Epoch 00007: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.6244 - acc: 0.3502 - val_loss: 1.6918 - val_acc: 0.3566\n",
      "Epoch 9/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5997 - acc: 0.3599Epoch 00008: val_loss improved from 1.64313 to 1.51403, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.5995 - acc: 0.3600 - val_loss: 1.5140 - val_acc: 0.4224\n",
      "Epoch 10/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5774 - acc: 0.3807Epoch 00009: val_loss improved from 1.51403 to 1.48215, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.5775 - acc: 0.3807 - val_loss: 1.4822 - val_acc: 0.4213\n",
      "Epoch 11/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5533 - acc: 0.3925Epoch 00010: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.5534 - acc: 0.3924 - val_loss: 1.4911 - val_acc: 0.4313\n",
      "Epoch 12/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5334 - acc: 0.4006Epoch 00011: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.5333 - acc: 0.4006 - val_loss: 1.4831 - val_acc: 0.4182\n",
      "Epoch 13/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5180 - acc: 0.4078Epoch 00012: val_loss improved from 1.48215 to 1.41118, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.5181 - acc: 0.4077 - val_loss: 1.4112 - val_acc: 0.4430\n",
      "Epoch 14/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5048 - acc: 0.4157Epoch 00013: val_loss improved from 1.41118 to 1.41018, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.5046 - acc: 0.4157 - val_loss: 1.4102 - val_acc: 0.4556\n",
      "Epoch 15/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4855 - acc: 0.4206Epoch 00014: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4851 - acc: 0.4207 - val_loss: 1.5947 - val_acc: 0.4065\n",
      "Epoch 16/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4750 - acc: 0.4302Epoch 00015: val_loss improved from 1.41018 to 1.33372, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.4752 - acc: 0.4301 - val_loss: 1.3337 - val_acc: 0.4882\n",
      "Epoch 17/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4556 - acc: 0.4380Epoch 00016: val_loss improved from 1.33372 to 1.33266, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.4556 - acc: 0.4380 - val_loss: 1.3327 - val_acc: 0.4770\n",
      "Epoch 18/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4486 - acc: 0.4362Epoch 00017: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4485 - acc: 0.4363 - val_loss: 1.3628 - val_acc: 0.4714\n",
      "Epoch 19/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4411 - acc: 0.4429Epoch 00018: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4412 - acc: 0.4429 - val_loss: 1.3501 - val_acc: 0.4742\n",
      "Epoch 20/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4298 - acc: 0.4497Epoch 00019: val_loss improved from 1.33266 to 1.31761, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.4303 - acc: 0.4494 - val_loss: 1.3176 - val_acc: 0.4851\n",
      "Epoch 21/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4127 - acc: 0.4547Epoch 00020: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4126 - acc: 0.4548 - val_loss: 1.3837 - val_acc: 0.4678\n",
      "Epoch 22/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4098 - acc: 0.4554Epoch 00021: val_loss improved from 1.31761 to 1.26879, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.4098 - acc: 0.4553 - val_loss: 1.2688 - val_acc: 0.5068\n",
      "Epoch 23/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4030 - acc: 0.4607Epoch 00022: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.4028 - acc: 0.4609 - val_loss: 1.2894 - val_acc: 0.5135\n",
      "Epoch 24/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3911 - acc: 0.4622Epoch 00023: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3910 - acc: 0.4624 - val_loss: 1.2832 - val_acc: 0.4957\n",
      "Epoch 25/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3787 - acc: 0.4712Epoch 00024: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3786 - acc: 0.4712 - val_loss: 1.2920 - val_acc: 0.4932\n",
      "Epoch 26/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3821 - acc: 0.4721Epoch 00025: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3823 - acc: 0.4719 - val_loss: 1.2836 - val_acc: 0.5060\n",
      "Epoch 27/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3671 - acc: 0.4758Epoch 00026: val_loss improved from 1.26879 to 1.23349, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.3669 - acc: 0.4759 - val_loss: 1.2335 - val_acc: 0.5247\n",
      "Epoch 28/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3545 - acc: 0.4784Epoch 00027: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3542 - acc: 0.4786 - val_loss: 1.2547 - val_acc: 0.5230\n",
      "Epoch 29/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3575 - acc: 0.4798Epoch 00028: val_loss improved from 1.23349 to 1.22357, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.3574 - acc: 0.4800 - val_loss: 1.2236 - val_acc: 0.5288\n",
      "Epoch 30/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3472 - acc: 0.4850Epoch 00029: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709/28709 [==============================] - 18s - loss: 1.3474 - acc: 0.4848 - val_loss: 1.2658 - val_acc: 0.5093\n",
      "Epoch 31/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3413 - acc: 0.4864Epoch 00030: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3412 - acc: 0.4864 - val_loss: 1.2532 - val_acc: 0.5135\n",
      "Epoch 32/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3351 - acc: 0.4915Epoch 00031: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3351 - acc: 0.4914 - val_loss: 1.2701 - val_acc: 0.5163\n",
      "Epoch 33/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3276 - acc: 0.4948Epoch 00032: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3281 - acc: 0.4947 - val_loss: 1.2322 - val_acc: 0.5361\n",
      "Epoch 34/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3239 - acc: 0.4932Epoch 00033: val_loss improved from 1.22357 to 1.21019, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.3238 - acc: 0.4932 - val_loss: 1.2102 - val_acc: 0.5391\n",
      "Epoch 35/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3220 - acc: 0.4918Epoch 00034: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3220 - acc: 0.4918 - val_loss: 1.3011 - val_acc: 0.5068\n",
      "Epoch 36/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3201 - acc: 0.4968Epoch 00035: val_loss improved from 1.21019 to 1.20821, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.3200 - acc: 0.4968 - val_loss: 1.2082 - val_acc: 0.5500\n",
      "Epoch 37/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3067 - acc: 0.5053Epoch 00036: val_loss improved from 1.20821 to 1.20426, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.3065 - acc: 0.5054 - val_loss: 1.2043 - val_acc: 0.5517\n",
      "Epoch 38/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3030 - acc: 0.5009Epoch 00037: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.3031 - acc: 0.5009 - val_loss: 1.2211 - val_acc: 0.5288\n",
      "Epoch 39/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2986 - acc: 0.5075Epoch 00038: val_loss improved from 1.20426 to 1.17752, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2984 - acc: 0.5077 - val_loss: 1.1775 - val_acc: 0.5486\n",
      "Epoch 40/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2935 - acc: 0.5051Epoch 00039: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2934 - acc: 0.5051 - val_loss: 1.2637 - val_acc: 0.5163\n",
      "Epoch 41/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2853 - acc: 0.5143Epoch 00040: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2851 - acc: 0.5143 - val_loss: 1.2156 - val_acc: 0.5283\n",
      "Epoch 42/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2881 - acc: 0.5170Epoch 00041: val_loss improved from 1.17752 to 1.17266, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2883 - acc: 0.5170 - val_loss: 1.1727 - val_acc: 0.5536\n",
      "Epoch 43/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2752 - acc: 0.5154Epoch 00042: val_loss improved from 1.17266 to 1.16975, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2755 - acc: 0.5154 - val_loss: 1.1697 - val_acc: 0.5573\n",
      "Epoch 44/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2654 - acc: 0.5219Epoch 00043: val_loss improved from 1.16975 to 1.15836, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2656 - acc: 0.5219 - val_loss: 1.1584 - val_acc: 0.5667\n",
      "Epoch 45/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2598 - acc: 0.5229Epoch 00044: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2595 - acc: 0.5232 - val_loss: 1.1962 - val_acc: 0.5380\n",
      "Epoch 46/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2628 - acc: 0.5243Epoch 00045: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2629 - acc: 0.5243 - val_loss: 1.1602 - val_acc: 0.5598\n",
      "Epoch 47/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2585 - acc: 0.5240Epoch 00046: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2588 - acc: 0.5238 - val_loss: 1.1678 - val_acc: 0.5548\n",
      "Epoch 48/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2508 - acc: 0.5269Epoch 00047: val_loss improved from 1.15836 to 1.14865, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2504 - acc: 0.5270 - val_loss: 1.1486 - val_acc: 0.5681\n",
      "Epoch 49/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2442 - acc: 0.5269Epoch 00048: val_loss improved from 1.14865 to 1.14007, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2444 - acc: 0.5267 - val_loss: 1.1401 - val_acc: 0.5706\n",
      "Epoch 50/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2379 - acc: 0.5320Epoch 00049: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2377 - acc: 0.5321 - val_loss: 1.1578 - val_acc: 0.5559\n",
      "Epoch 51/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2372 - acc: 0.5340Epoch 00050: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2375 - acc: 0.5340 - val_loss: 1.1868 - val_acc: 0.5458\n",
      "Epoch 52/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2295 - acc: 0.5357Epoch 00051: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2294 - acc: 0.5357 - val_loss: 1.1914 - val_acc: 0.5522\n",
      "Epoch 53/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2278 - acc: 0.5363Epoch 00052: val_loss improved from 1.14007 to 1.13712, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2285 - acc: 0.5360 - val_loss: 1.1371 - val_acc: 0.5692\n",
      "Epoch 54/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2257 - acc: 0.5419Epoch 00053: val_loss improved from 1.13712 to 1.12887, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2262 - acc: 0.5417 - val_loss: 1.1289 - val_acc: 0.5751\n",
      "Epoch 55/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2175 - acc: 0.5407Epoch 00054: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2173 - acc: 0.5407 - val_loss: 1.1344 - val_acc: 0.5762\n",
      "Epoch 56/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2172 - acc: 0.5462Epoch 00055: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2170 - acc: 0.5462 - val_loss: 1.1742 - val_acc: 0.5578\n",
      "Epoch 57/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2103 - acc: 0.5457Epoch 00056: val_loss improved from 1.12887 to 1.12841, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2101 - acc: 0.5456 - val_loss: 1.1284 - val_acc: 0.5731\n",
      "Epoch 58/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2040 - acc: 0.5471Epoch 00057: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2038 - acc: 0.5471 - val_loss: 1.1613 - val_acc: 0.5628\n",
      "Epoch 59/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2035 - acc: 0.5494Epoch 00058: val_loss improved from 1.12841 to 1.11781, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.2036 - acc: 0.5492 - val_loss: 1.1178 - val_acc: 0.5804\n",
      "Epoch 60/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2009 - acc: 0.5506Epoch 00059: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.2005 - acc: 0.5507 - val_loss: 1.1517 - val_acc: 0.5648\n",
      "Epoch 61/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1987 - acc: 0.5520Epoch 00060: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1984 - acc: 0.5521 - val_loss: 1.1718 - val_acc: 0.5575\n",
      "Epoch 62/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1930 - acc: 0.5533Epoch 00061: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1928 - acc: 0.5534 - val_loss: 1.2265 - val_acc: 0.5391\n",
      "Epoch 63/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1868 - acc: 0.5575Epoch 00062: val_loss improved from 1.11781 to 1.11694, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.1869 - acc: 0.5574 - val_loss: 1.1169 - val_acc: 0.5793\n",
      "Epoch 64/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1806 - acc: 0.5562Epoch 00063: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1805 - acc: 0.5562 - val_loss: 1.1311 - val_acc: 0.5712\n",
      "Epoch 65/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1768 - acc: 0.5561Epoch 00064: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1768 - acc: 0.5560 - val_loss: 1.1307 - val_acc: 0.5812\n",
      "Epoch 66/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1797 - acc: 0.5562Epoch 00065: val_loss improved from 1.11694 to 1.11235, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.1796 - acc: 0.5561 - val_loss: 1.1124 - val_acc: 0.5899\n",
      "Epoch 67/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1754 - acc: 0.5593Epoch 00066: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1756 - acc: 0.5591 - val_loss: 1.1618 - val_acc: 0.5673\n",
      "Epoch 68/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1671 - acc: 0.5625Epoch 00067: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1671 - acc: 0.5625 - val_loss: 1.1346 - val_acc: 0.5773\n",
      "Epoch 69/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1657 - acc: 0.5637Epoch 00068: val_loss improved from 1.11235 to 1.10299, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 19s - loss: 1.1654 - acc: 0.5638 - val_loss: 1.1030 - val_acc: 0.5821\n",
      "Epoch 70/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1651 - acc: 0.5644Epoch 00069: val_loss improved from 1.10299 to 1.09918, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 19s - loss: 1.1649 - acc: 0.5645 - val_loss: 1.0992 - val_acc: 0.5915\n",
      "Epoch 71/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1567 - acc: 0.5679Epoch 00070: val_loss improved from 1.09918 to 1.09503, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 19s - loss: 1.1564 - acc: 0.5679 - val_loss: 1.0950 - val_acc: 0.5882\n",
      "Epoch 72/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1607 - acc: 0.5658Epoch 00071: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1607 - acc: 0.5660 - val_loss: 1.0997 - val_acc: 0.5854\n",
      "Epoch 73/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1569 - acc: 0.5674Epoch 00072: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1567 - acc: 0.5675 - val_loss: 1.1029 - val_acc: 0.5896\n",
      "Epoch 74/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1530 - acc: 0.5660Epoch 00073: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1529 - acc: 0.5658 - val_loss: 1.1290 - val_acc: 0.5784\n",
      "Epoch 75/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1433 - acc: 0.5710Epoch 00074: val_loss improved from 1.09503 to 1.07989, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.1439 - acc: 0.5709 - val_loss: 1.0799 - val_acc: 0.6030\n",
      "Epoch 76/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1423 - acc: 0.5699Epoch 00075: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1422 - acc: 0.5699 - val_loss: 1.1058 - val_acc: 0.5871\n",
      "Epoch 77/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1459 - acc: 0.5735Epoch 00076: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1459 - acc: 0.5734 - val_loss: 1.0846 - val_acc: 0.5879\n",
      "Epoch 78/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1386 - acc: 0.5737Epoch 00077: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1387 - acc: 0.5735 - val_loss: 1.1080 - val_acc: 0.5868\n",
      "Epoch 79/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1327 - acc: 0.5727Epoch 00078: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1326 - acc: 0.5727 - val_loss: 1.0891 - val_acc: 0.5974\n",
      "Epoch 80/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1347 - acc: 0.5776Epoch 00079: val_loss improved from 1.07989 to 1.07296, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 19s - loss: 1.1349 - acc: 0.5776 - val_loss: 1.0730 - val_acc: 0.6032\n",
      "Epoch 81/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1216 - acc: 0.5819Epoch 00080: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1218 - acc: 0.5819 - val_loss: 1.0955 - val_acc: 0.5876\n",
      "Epoch 82/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1235 - acc: 0.5846Epoch 00081: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1238 - acc: 0.5845 - val_loss: 1.1167 - val_acc: 0.5823\n",
      "Epoch 83/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1279 - acc: 0.5823Epoch 00082: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1280 - acc: 0.5822 - val_loss: 1.0803 - val_acc: 0.6010\n",
      "Epoch 84/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1191 - acc: 0.5809Epoch 00083: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1189 - acc: 0.5809 - val_loss: 1.0921 - val_acc: 0.5887\n",
      "Epoch 85/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1193 - acc: 0.5807Epoch 00084: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1192 - acc: 0.5808 - val_loss: 1.0910 - val_acc: 0.5952\n",
      "Epoch 86/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1048 - acc: 0.5884Epoch 00085: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1046 - acc: 0.5885 - val_loss: 1.1648 - val_acc: 0.5634\n",
      "Epoch 87/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1092 - acc: 0.5876Epoch 00086: val_loss improved from 1.07296 to 1.07224, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.1091 - acc: 0.5877 - val_loss: 1.0722 - val_acc: 0.6063\n",
      "Epoch 88/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1016 - acc: 0.5869Epoch 00087: val_loss improved from 1.07224 to 1.06699, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.1016 - acc: 0.5869 - val_loss: 1.0670 - val_acc: 0.6046\n",
      "Epoch 89/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1060 - acc: 0.5877Epoch 00088: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1058 - acc: 0.5878 - val_loss: 1.0794 - val_acc: 0.5982\n",
      "Epoch 90/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1005 - acc: 0.5920Epoch 00089: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.1003 - acc: 0.5921 - val_loss: 1.0931 - val_acc: 0.5940\n",
      "Epoch 91/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0970 - acc: 0.5901Epoch 00090: val_loss improved from 1.06699 to 1.06250, saving model to face_model4.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709/28709 [==============================] - 18s - loss: 1.0970 - acc: 0.5901 - val_loss: 1.0625 - val_acc: 0.5971\n",
      "Epoch 92/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0964 - acc: 0.5951Epoch 00091: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.0966 - acc: 0.5951 - val_loss: 1.0771 - val_acc: 0.6021\n",
      "Epoch 93/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0899 - acc: 0.5962Epoch 00092: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.0899 - acc: 0.5961 - val_loss: 1.0798 - val_acc: 0.5946\n",
      "Epoch 94/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0873 - acc: 0.5919Epoch 00093: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.0878 - acc: 0.5918 - val_loss: 1.0633 - val_acc: 0.6060\n",
      "Epoch 95/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0899 - acc: 0.5942Epoch 00094: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.0901 - acc: 0.5940 - val_loss: 1.0889 - val_acc: 0.5896\n",
      "Epoch 96/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0815 - acc: 0.5957Epoch 00095: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.0821 - acc: 0.5955 - val_loss: 1.1493 - val_acc: 0.5720\n",
      "Epoch 97/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0792 - acc: 0.5959Epoch 00096: val_loss improved from 1.06250 to 1.04642, saving model to face_model4.h5\n",
      "28709/28709 [==============================] - 18s - loss: 1.0792 - acc: 0.5958 - val_loss: 1.0464 - val_acc: 0.6116\n",
      "Epoch 98/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0703 - acc: 0.5970Epoch 00097: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.0702 - acc: 0.5972 - val_loss: 1.0694 - val_acc: 0.6046\n",
      "Epoch 99/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0751 - acc: 0.6012Epoch 00098: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.0751 - acc: 0.6012 - val_loss: 1.0830 - val_acc: 0.6016\n",
      "Epoch 100/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0693 - acc: 0.6025Epoch 00099: val_loss did not improve\n",
      "28709/28709 [==============================] - 18s - loss: 1.0692 - acc: 0.6025 - val_loss: 1.1655 - val_acc: 0.5678\n"
     ]
    }
   ],
   "source": [
    "print(\"Model no :\", 4)\n",
    "cur_model = model_list[3].fit(X_train, y_train,\n",
    "                    batch_size=batch_size,epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer[3]])\n",
    "train_model.append(cur_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model no : 5\n",
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 3.0397 - acc: 0.2065Epoch 00000: val_loss improved from inf to 1.85513, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 90s - loss: 3.0380 - acc: 0.2065 - val_loss: 1.8551 - val_acc: 0.1962\n",
      "Epoch 2/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.9255 - acc: 0.2479Epoch 00001: val_loss improved from 1.85513 to 1.80264, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.9254 - acc: 0.2480 - val_loss: 1.8026 - val_acc: 0.2611\n",
      "Epoch 3/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.7639 - acc: 0.2881Epoch 00002: val_loss improved from 1.80264 to 1.66330, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.7639 - acc: 0.2881 - val_loss: 1.6633 - val_acc: 0.2909\n",
      "Epoch 4/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6787 - acc: 0.3271Epoch 00003: val_loss did not improve\n",
      "28709/28709 [==============================] - 76s - loss: 1.6788 - acc: 0.3271 - val_loss: 1.6778 - val_acc: 0.3391\n",
      "Epoch 5/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.6168 - acc: 0.3628Epoch 00004: val_loss improved from 1.66330 to 1.65710, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.6171 - acc: 0.3627 - val_loss: 1.6571 - val_acc: 0.3402\n",
      "Epoch 6/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5796 - acc: 0.3748Epoch 00005: val_loss improved from 1.65710 to 1.55692, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.5795 - acc: 0.3748 - val_loss: 1.5569 - val_acc: 0.3795\n",
      "Epoch 7/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5453 - acc: 0.3906Epoch 00006: val_loss improved from 1.55692 to 1.48728, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.5453 - acc: 0.3906 - val_loss: 1.4873 - val_acc: 0.4104\n",
      "Epoch 8/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.5036 - acc: 0.4083Epoch 00007: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.5036 - acc: 0.4083 - val_loss: 1.4920 - val_acc: 0.4026\n",
      "Epoch 9/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4795 - acc: 0.4211Epoch 00008: val_loss improved from 1.48728 to 1.45168, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.4795 - acc: 0.4212 - val_loss: 1.4517 - val_acc: 0.4374\n",
      "Epoch 10/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4509 - acc: 0.4350Epoch 00009: val_loss improved from 1.45168 to 1.41456, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.4511 - acc: 0.4349 - val_loss: 1.4146 - val_acc: 0.4249\n",
      "Epoch 11/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4336 - acc: 0.4438Epoch 00010: val_loss did not improve\n",
      "28709/28709 [==============================] - 76s - loss: 1.4337 - acc: 0.4437 - val_loss: 1.4735 - val_acc: 0.4202\n",
      "Epoch 12/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.4151 - acc: 0.4474Epoch 00011: val_loss improved from 1.41456 to 1.40726, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.4153 - acc: 0.4474 - val_loss: 1.4073 - val_acc: 0.4316\n",
      "Epoch 13/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3904 - acc: 0.4583Epoch 00012: val_loss improved from 1.40726 to 1.38935, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.3904 - acc: 0.4583 - val_loss: 1.3893 - val_acc: 0.4439\n",
      "Epoch 14/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3660 - acc: 0.4721Epoch 00013: val_loss improved from 1.38935 to 1.35693, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.3662 - acc: 0.4719 - val_loss: 1.3569 - val_acc: 0.4687\n",
      "Epoch 15/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3506 - acc: 0.4786Epoch 00014: val_loss improved from 1.35693 to 1.33791, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.3507 - acc: 0.4789 - val_loss: 1.3379 - val_acc: 0.4826\n",
      "Epoch 16/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3303 - acc: 0.4851Epoch 00015: val_loss improved from 1.33791 to 1.32305, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.3301 - acc: 0.4852 - val_loss: 1.3231 - val_acc: 0.4848\n",
      "Epoch 17/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3130 - acc: 0.4934Epoch 00016: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.3133 - acc: 0.4933 - val_loss: 1.4145 - val_acc: 0.4305\n",
      "Epoch 18/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.3000 - acc: 0.4960Epoch 00017: val_loss improved from 1.32305 to 1.31936, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.3001 - acc: 0.4959 - val_loss: 1.3194 - val_acc: 0.4999\n",
      "Epoch 19/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2886 - acc: 0.5063Epoch 00018: val_loss improved from 1.31936 to 1.29003, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.2888 - acc: 0.5061 - val_loss: 1.2900 - val_acc: 0.4943\n",
      "Epoch 20/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2667 - acc: 0.5120Epoch 00019: val_loss improved from 1.29003 to 1.25646, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.2665 - acc: 0.5120 - val_loss: 1.2565 - val_acc: 0.5205\n",
      "Epoch 21/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2497 - acc: 0.5206Epoch 00020: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.2498 - acc: 0.5206 - val_loss: 1.2591 - val_acc: 0.5163\n",
      "Epoch 22/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2363 - acc: 0.5301Epoch 00021: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.2366 - acc: 0.5299 - val_loss: 1.2667 - val_acc: 0.5096\n",
      "Epoch 23/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2198 - acc: 0.5323Epoch 00022: val_loss improved from 1.25646 to 1.25004, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.2196 - acc: 0.5323 - val_loss: 1.2500 - val_acc: 0.5191\n",
      "Epoch 24/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.2086 - acc: 0.5375Epoch 00023: val_loss improved from 1.25004 to 1.23716, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.2087 - acc: 0.5374 - val_loss: 1.2372 - val_acc: 0.5230\n",
      "Epoch 25/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1904 - acc: 0.5461Epoch 00024: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.1901 - acc: 0.5463 - val_loss: 1.2611 - val_acc: 0.5210\n",
      "Epoch 26/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1808 - acc: 0.5496Epoch 00025: val_loss improved from 1.23716 to 1.20217, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.1807 - acc: 0.5497 - val_loss: 1.2022 - val_acc: 0.5536\n",
      "Epoch 27/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1678 - acc: 0.5568Epoch 00026: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.1677 - acc: 0.5569 - val_loss: 1.2128 - val_acc: 0.5366\n",
      "Epoch 28/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1537 - acc: 0.5595Epoch 00027: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.1536 - acc: 0.5594 - val_loss: 1.2238 - val_acc: 0.5199\n",
      "Epoch 29/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1422 - acc: 0.5697Epoch 00028: val_loss improved from 1.20217 to 1.19276, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.1424 - acc: 0.5696 - val_loss: 1.1928 - val_acc: 0.5481\n",
      "Epoch 30/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1201 - acc: 0.5721Epoch 00029: val_loss improved from 1.19276 to 1.18623, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.1204 - acc: 0.5720 - val_loss: 1.1862 - val_acc: 0.5467\n",
      "Epoch 31/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.1008 - acc: 0.5822Epoch 00030: val_loss improved from 1.18623 to 1.15792, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.1004 - acc: 0.5824 - val_loss: 1.1579 - val_acc: 0.5653\n",
      "Epoch 32/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0922 - acc: 0.5867Epoch 00031: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.0922 - acc: 0.5866 - val_loss: 1.1664 - val_acc: 0.5570\n",
      "Epoch 33/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0775 - acc: 0.5932Epoch 00032: val_loss improved from 1.15792 to 1.13284, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.0773 - acc: 0.5933 - val_loss: 1.1328 - val_acc: 0.5756\n",
      "Epoch 34/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0652 - acc: 0.6010Epoch 00033: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.0657 - acc: 0.6009 - val_loss: 1.1574 - val_acc: 0.5653\n",
      "Epoch 35/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0524 - acc: 0.6053Epoch 00034: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.0527 - acc: 0.6052 - val_loss: 1.1750 - val_acc: 0.5614\n",
      "Epoch 36/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0379 - acc: 0.6087Epoch 00035: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.0376 - acc: 0.6088 - val_loss: 1.1659 - val_acc: 0.5687\n",
      "Epoch 37/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0233 - acc: 0.6131Epoch 00036: val_loss improved from 1.13284 to 1.13003, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 1.0241 - acc: 0.6130 - val_loss: 1.1300 - val_acc: 0.5790\n",
      "Epoch 38/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 1.0071 - acc: 0.6251Epoch 00037: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 1.0073 - acc: 0.6250 - val_loss: 1.1335 - val_acc: 0.5642\n",
      "Epoch 39/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9959 - acc: 0.6276Epoch 00038: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.9960 - acc: 0.6276 - val_loss: 1.1350 - val_acc: 0.5698\n",
      "Epoch 40/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9844 - acc: 0.6325Epoch 00039: val_loss improved from 1.13003 to 1.12332, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 0.9846 - acc: 0.6323 - val_loss: 1.1233 - val_acc: 0.5779\n",
      "Epoch 41/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9754 - acc: 0.6364Epoch 00040: val_loss improved from 1.12332 to 1.09500, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 0.9756 - acc: 0.6362 - val_loss: 1.0950 - val_acc: 0.5851\n",
      "Epoch 42/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9620 - acc: 0.6396Epoch 00041: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.9622 - acc: 0.6395 - val_loss: 1.1226 - val_acc: 0.5704\n",
      "Epoch 43/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9485 - acc: 0.6482Epoch 00042: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.9481 - acc: 0.6483 - val_loss: 1.1298 - val_acc: 0.5776\n",
      "Epoch 44/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9298 - acc: 0.6571Epoch 00043: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.9295 - acc: 0.6572 - val_loss: 1.1700 - val_acc: 0.5628\n",
      "Epoch 45/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9226 - acc: 0.6598Epoch 00044: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.9224 - acc: 0.6599 - val_loss: 1.1019 - val_acc: 0.5926\n",
      "Epoch 46/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9003 - acc: 0.6653Epoch 00045: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.9007 - acc: 0.6652 - val_loss: 1.1267 - val_acc: 0.5893\n",
      "Epoch 47/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.9030 - acc: 0.6669Epoch 00046: val_loss improved from 1.09500 to 1.09477, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 0.9027 - acc: 0.6670 - val_loss: 1.0948 - val_acc: 0.5965\n",
      "Epoch 48/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8910 - acc: 0.6738Epoch 00047: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.8909 - acc: 0.6739 - val_loss: 1.1148 - val_acc: 0.5896\n",
      "Epoch 49/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8692 - acc: 0.6797Epoch 00048: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.8694 - acc: 0.6795 - val_loss: 1.1038 - val_acc: 0.5926\n",
      "Epoch 50/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8600 - acc: 0.6854Epoch 00049: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.8604 - acc: 0.6853 - val_loss: 1.1041 - val_acc: 0.5926\n",
      "Epoch 51/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8535 - acc: 0.6865Epoch 00050: val_loss improved from 1.09477 to 1.08494, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 0.8537 - acc: 0.6863 - val_loss: 1.0849 - val_acc: 0.5979\n",
      "Epoch 52/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8509 - acc: 0.6907Epoch 00051: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.8513 - acc: 0.6907 - val_loss: 1.0889 - val_acc: 0.6010\n",
      "Epoch 53/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8335 - acc: 0.6938Epoch 00052: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.8339 - acc: 0.6939 - val_loss: 1.1653 - val_acc: 0.5854\n",
      "Epoch 54/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8204 - acc: 0.6996Epoch 00053: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.8205 - acc: 0.6995 - val_loss: 1.0893 - val_acc: 0.6074\n",
      "Epoch 55/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8076 - acc: 0.7068Epoch 00054: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.8080 - acc: 0.7065 - val_loss: 1.1217 - val_acc: 0.5890\n",
      "Epoch 56/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.8004 - acc: 0.7111Epoch 00055: val_loss improved from 1.08494 to 1.07248, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 0.8005 - acc: 0.7111 - val_loss: 1.0725 - val_acc: 0.6049\n",
      "Epoch 57/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7957 - acc: 0.7093Epoch 00056: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7958 - acc: 0.7092 - val_loss: 1.0728 - val_acc: 0.6060\n",
      "Epoch 58/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7877 - acc: 0.7132Epoch 00057: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7876 - acc: 0.7132 - val_loss: 1.0822 - val_acc: 0.5985\n",
      "Epoch 59/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7724 - acc: 0.7158Epoch 00058: val_loss improved from 1.07248 to 1.06626, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 0.7725 - acc: 0.7157 - val_loss: 1.0663 - val_acc: 0.6088\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7627 - acc: 0.7221Epoch 00059: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7626 - acc: 0.7221 - val_loss: 1.0884 - val_acc: 0.6057\n",
      "Epoch 61/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7523 - acc: 0.7260Epoch 00060: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7522 - acc: 0.7260 - val_loss: 1.0802 - val_acc: 0.6052\n",
      "Epoch 62/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7387 - acc: 0.7335Epoch 00061: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7387 - acc: 0.7335 - val_loss: 1.1009 - val_acc: 0.6160\n",
      "Epoch 63/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7501 - acc: 0.7286Epoch 00062: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7501 - acc: 0.7286 - val_loss: 1.0864 - val_acc: 0.6010\n",
      "Epoch 64/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7434 - acc: 0.7328Epoch 00063: val_loss did not improve\n",
      "28709/28709 [==============================] - 76s - loss: 0.7434 - acc: 0.7328 - val_loss: 1.0840 - val_acc: 0.6108\n",
      "Epoch 65/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7233 - acc: 0.7401Epoch 00064: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7232 - acc: 0.7400 - val_loss: 1.1042 - val_acc: 0.6135\n",
      "Epoch 66/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7082 - acc: 0.7439Epoch 00065: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7082 - acc: 0.7438 - val_loss: 1.0773 - val_acc: 0.6091\n",
      "Epoch 67/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7018 - acc: 0.7473Epoch 00066: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7017 - acc: 0.7472 - val_loss: 1.0772 - val_acc: 0.6074\n",
      "Epoch 68/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7000 - acc: 0.7483Epoch 00067: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7002 - acc: 0.7483 - val_loss: 1.0963 - val_acc: 0.6113\n",
      "Epoch 69/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.7024 - acc: 0.7514Epoch 00068: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.7021 - acc: 0.7515 - val_loss: 1.0853 - val_acc: 0.6172\n",
      "Epoch 70/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6794 - acc: 0.7544Epoch 00069: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6792 - acc: 0.7545 - val_loss: 1.0843 - val_acc: 0.6141\n",
      "Epoch 71/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6651 - acc: 0.7608Epoch 00070: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6651 - acc: 0.7607 - val_loss: 1.0982 - val_acc: 0.6147\n",
      "Epoch 72/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6728 - acc: 0.7593Epoch 00071: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6726 - acc: 0.7593 - val_loss: 1.1135 - val_acc: 0.6163\n",
      "Epoch 73/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6607 - acc: 0.7637Epoch 00072: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6608 - acc: 0.7637 - val_loss: 1.0845 - val_acc: 0.6172\n",
      "Epoch 74/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6554 - acc: 0.7673Epoch 00073: val_loss improved from 1.06626 to 1.05670, saving model to face_model5.h5\n",
      "28709/28709 [==============================] - 76s - loss: 0.6554 - acc: 0.7672 - val_loss: 1.0567 - val_acc: 0.6186\n",
      "Epoch 75/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6491 - acc: 0.7683Epoch 00074: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6496 - acc: 0.7682 - val_loss: 1.1051 - val_acc: 0.6191\n",
      "Epoch 76/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6377 - acc: 0.7721Epoch 00075: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6378 - acc: 0.7721 - val_loss: 1.1025 - val_acc: 0.6188\n",
      "Epoch 77/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6460 - acc: 0.7689Epoch 00076: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6458 - acc: 0.7690 - val_loss: 1.0978 - val_acc: 0.6252\n",
      "Epoch 78/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6285 - acc: 0.7747Epoch 00077: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6287 - acc: 0.7746 - val_loss: 1.0890 - val_acc: 0.6252\n",
      "Epoch 79/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6401 - acc: 0.7761Epoch 00078: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6403 - acc: 0.7760 - val_loss: 1.1002 - val_acc: 0.6205\n",
      "Epoch 80/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6125 - acc: 0.7804Epoch 00079: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6123 - acc: 0.7805 - val_loss: 1.1100 - val_acc: 0.6074\n",
      "Epoch 81/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6102 - acc: 0.7823Epoch 00080: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6100 - acc: 0.7824 - val_loss: 1.0798 - val_acc: 0.6219\n",
      "Epoch 82/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6065 - acc: 0.7867Epoch 00081: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6071 - acc: 0.7865 - val_loss: 1.1092 - val_acc: 0.6227\n",
      "Epoch 83/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5970 - acc: 0.7879Epoch 00082: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5971 - acc: 0.7880 - val_loss: 1.1175 - val_acc: 0.6166\n",
      "Epoch 84/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5977 - acc: 0.7913Epoch 00083: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5980 - acc: 0.7911 - val_loss: 1.1016 - val_acc: 0.6135\n",
      "Epoch 85/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.6071 - acc: 0.7876Epoch 00084: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.6069 - acc: 0.7877 - val_loss: 1.1110 - val_acc: 0.6177\n",
      "Epoch 86/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5838 - acc: 0.7927Epoch 00085: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5838 - acc: 0.7927 - val_loss: 1.0974 - val_acc: 0.6264\n",
      "Epoch 87/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5964 - acc: 0.7889Epoch 00086: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5964 - acc: 0.7889 - val_loss: 1.0996 - val_acc: 0.6236\n",
      "Epoch 88/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5857 - acc: 0.7931Epoch 00087: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5861 - acc: 0.7931 - val_loss: 1.1103 - val_acc: 0.6305\n",
      "Epoch 89/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5706 - acc: 0.7964Epoch 00088: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5704 - acc: 0.7964 - val_loss: 1.1262 - val_acc: 0.6275\n",
      "Epoch 90/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.7995Epoch 00089: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5722 - acc: 0.7994 - val_loss: 1.0930 - val_acc: 0.6291\n",
      "Epoch 91/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5621 - acc: 0.8029Epoch 00090: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5623 - acc: 0.8028 - val_loss: 1.1041 - val_acc: 0.6266\n",
      "Epoch 92/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5589 - acc: 0.8041Epoch 00091: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709/28709 [==============================] - 75s - loss: 0.5587 - acc: 0.8041 - val_loss: 1.1152 - val_acc: 0.6280\n",
      "Epoch 93/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.8056Epoch 00092: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5526 - acc: 0.8055 - val_loss: 1.0966 - val_acc: 0.6261\n",
      "Epoch 94/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5369 - acc: 0.8104Epoch 00093: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5373 - acc: 0.8103 - val_loss: 1.0834 - val_acc: 0.6289\n",
      "Epoch 95/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5528 - acc: 0.8042Epoch 00094: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5527 - acc: 0.8042 - val_loss: 1.0878 - val_acc: 0.6283\n",
      "Epoch 96/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5488 - acc: 0.8068Epoch 00095: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5491 - acc: 0.8068 - val_loss: 1.1180 - val_acc: 0.6222\n",
      "Epoch 97/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5347 - acc: 0.8127Epoch 00096: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5344 - acc: 0.8127 - val_loss: 1.1257 - val_acc: 0.6213\n",
      "Epoch 98/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5370 - acc: 0.8110Epoch 00097: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5373 - acc: 0.8108 - val_loss: 1.0968 - val_acc: 0.6261\n",
      "Epoch 99/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5196 - acc: 0.8176Epoch 00098: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5197 - acc: 0.8176 - val_loss: 1.1072 - val_acc: 0.6241\n",
      "Epoch 100/100\n",
      "28672/28709 [============================>.] - ETA: 0s - loss: 0.5238 - acc: 0.8170Epoch 00099: val_loss did not improve\n",
      "28709/28709 [==============================] - 75s - loss: 0.5235 - acc: 0.8170 - val_loss: 1.1031 - val_acc: 0.6330\n"
     ]
    }
   ],
   "source": [
    "print(\"Model no :\", 5)\n",
    "cur_model = model_list[4].fit(X_train, y_train,\n",
    "                    batch_size=batch_size,epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpointer[4]])\n",
    "train_model.append(cur_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = []\n",
    "for ix in range(len(model_list)-1):\n",
    "    model_json.append(model_list[ix].to_json())\n",
    "    json_path = \"face_model\" + str(ix+1) + \".json\"\n",
    "    with open(json_path, \"w\") as json_file:\n",
    "        json_file.write(model_json[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for ix in range(5):\n",
    "    y_cur = model_list[ix].predict(X_test, verbose=0)\n",
    "    y_pred.append(y_cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.41905823349123\n"
     ]
    }
   ],
   "source": [
    "#Winning expression by max value\n",
    "correct = 0.0\n",
    "for ix in range(len(X_test)):\n",
    "    max_idx, max_val = -1, -1\n",
    "    for iy in range(5):\n",
    "        cur_arr = np.array(y_pred[iy][ix])\n",
    "        x = np.argmax(cur_arr)\n",
    "        if max_val < cur_arr[x]:\n",
    "            max_idx, max_val = x, cur_arr[x]\n",
    "    if max_idx == np.argmax(y_test[ix]):\n",
    "        correct += 1\n",
    "correct = correct/len(X_test)\n",
    "print(correct*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.25188074672612\n"
     ]
    }
   ],
   "source": [
    "#Winning value by maximum number of selection by different models\n",
    "correct = 0.0\n",
    "for ix in range(len(X_test)):\n",
    "    val_count = np.zeros(shape=(7, ))\n",
    "    for iy in range(5):\n",
    "        cur_arr = np.array(y_pred[iy][ix])\n",
    "        x = np.argmax(cur_arr)\n",
    "        val_count[x] += 1\n",
    "    \n",
    "    #print(max_idx, np.argmax(val_count))    \n",
    "    if np.argmax(y_test[ix]) == np.argmax(val_count):\n",
    "        correct += 1\n",
    "correct = correct/len(X_test)\n",
    "print(correct*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
